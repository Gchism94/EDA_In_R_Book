# Transforming like a Data... Transformer {.unnumbered}

```{r setup, include = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

## Purpose of this chapter

**Using data transformation to correct non-normality in numerical data**

------------------------------------------------------------------------

## Take-aways

1.  Load and explore a data set with publication quality tables
2.  Quickly diagnose non-normality in data
3.  Data transformation
4.  Prepare an HTML summary report showcasing data transformations

------------------------------------------------------------------------

## Required Setup

We first need to prepare our environment with the necessary packages

```{r req-setup, results = 'hide'}
# Sets the repository to download packages from
options(repos = list(CRAN = "http://cran.rstudio.com/"))

# Sets the number of significant figures to two - e.g., 0.01
options(digits = 2)

# Required package for quick package downloading and loading 
install.packages("pacman")

# Downloads and load required packages
pacman::p_load(dlookr, # Exploratory data analysis
               forecast, # Needed for Box-Cox transformations
               formattable, # HTML tables from R outputs
               here, # Standardizes paths to data
               kableExtra, # Alternative to formattable
               knitr, # Needed to write HTML reports
               missRanger, # To generate NAs
               tidyverse) # Powerful data wrangling package suite
```

------------------------------------------------------------------------

## Load and Examine a Data Set

-   Load data and view
-   Examine columns and data types
-   Examine data normality
-   Describe properties of data

```{r load-data}
# Let's load a data set from the diabetes data set
dataset <- read.csv(here("Data7_EDA_In_R_Book", "data", "diabetes.csv")) |>
  # Add a categorical group
  mutate(Age_group = ifelse(Age >= 21 & Age <= 30, "Young", 
                            ifelse(Age > 30 & Age <= 50, "Middle", 
                                   "Elderly")),
         Age_group = fct_rev(Age_group))

# What does the data look like?
dataset |>
  head() |>
  formattable()
```

------------------------------------------------------------------------

### Data Normality

Normal distributions (bell curves) are a common data assumptions for many [hypothesis testing statistics](https://en.wikipedia.org/wiki/Statistical_hypothesis_testing), in particular [parametric statistics](https://en.wikipedia.org/wiki/Parametric_statistics). Deviations from normality can either strongly skew the results or reduce the power to detect a [significant statistical difference](https://en.wikipedia.org/wiki/Statistical_significance).

Here are the distribution properties to know and consider:

-   The mean, median, and mode are the same value.

-   Distribution symmetry at the mean.

-   Normal distributions can be described by the mean and standard deviation.

Here's an example using the `Glucose` column in our `dataset`

```{r norm-plot-cent-tend}
library(ggpubr)

# Function for data mode
getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}

dataset |> 
  ggplot(aes(x = BMI)) +
  geom_histogram(fill = "#4E84C4", size = 2, bins = 40) +
  geom_point(aes(y = 30, x = mean(BMI)), size = 5) +
  geom_point(aes(y = 30, x = median(BMI)), size = 5) +
  geom_point(aes(y = 30, x = getmode(BMI)), size = 5) +
  geom_vline(aes(xintercept = mean(BMI) - sd(BMI)), size = 1.5, linetype = "dashed") + 
  geom_vline(aes(xintercept = mean(BMI) + sd(BMI)), size = 1.5, linetype = "dashed") +
  geom_segment(aes(y = 30, yend = 30, x = mean(BMI) - sd(BMI), xend = mean(BMI) + sd(BMI)), size = 1.5) +
  geom_segment(aes(y = 32, yend = 45, x = mean(BMI) + 1.5, xend = 50)) +
  geom_segment(aes(y = 30, yend = 30, x = mean(BMI) + sd(BMI) + 1, xend = 50)) +
  annotate(geom = "text", x = 50.5, y = 45, label = "Mean, \nMedian, \nMode = 32", size = 5, hjust = 0) +
  annotate(geom = "text", x = 50.5, y = 30, label = "SD = 7.9", size = 5, hjust = 0) +
  ylab("Count") +
  theme(axis.title.y = element_blank()) +
  theme_pubclean(base_size = 16)
```

------------------------------------------------------------------------

### Describing Properties of our Data (Refined)

#### Skewness

The symmetry of the distribution

See [Introduction -@sec-DistShape] for more information about these values

```{r describe-skew}
dataset |>
  select(Glucose, Insulin, BMI, SkinThickness) |>
  describe() |>
  select(described_variables, skewness) |>
  formattable()
```

Note that we will remove the other percentiles to produce a cleaner output

-   `describes_variables`: name of the column being described

-   `skewness`: skewness

------------------------------------------------------------------------

## Testing Normality (Accelerated)

-   Q-Q plots

-   Testing overall normality of two columns

-   Testing normality of groups

**Note** that you can also use `normality()` to run Shapiro-Wilk tests, but since this test is not viable at `N < 20`, I recommend Q-Q plots.

------------------------------------------------------------------------

#### Q-Q Plots

Plots of the quartiles of a target data set against the predicted quartiles from a normal distribution.

Notably, `plot_normality()` will show you the logaritmic and skewed transformations (more below)

```{r norm-plot}
dataset |>
plot_normality(Glucose, Insulin, Age)
```

------------------------------------------------------------------------

## Normality within Groups

Looking within Age_group at the subgroup normality

#### Q-Q Plots

```{r grouped-norm-plot}
dataset %>%
  group_by(Age_group) %>%
  select(Glucose, Insulin) %>%
  plot_normality()
```

------------------------------------------------------------------------

## Transforming Data

Your data could be more easily interpreted with a transformation, since not all relationships in nature follow a linear relationship - i.e., many biological phenomena follow a power law (or logarithmic curve), where they do not scale linearly.

We will try to transform the `Insulin` column with through several approaches and discuss the pros and cons of each. First however, we will remove `0` values, because `Insulin` values are impossible...

```{r mod-insulin}
InsMod <- dataset |>
  filter(Insulin > 0)
```

------------------------------------------------------------------------

### Square-root, Cube-root, and Logarithmic Transformations

Resolving Skewness using `transform()`.

"sqrt": [square-root transformation](https://en.wikipedia.org/wiki/Square_root). $\sqrt x$ **(moderate skew)**

"log": [log transformation](https://en.wikipedia.org/wiki/Logarithm). $log(x)$ **(greater skew)**

"log+1": log transformation. $log(x + 1)$. Used for values that contain 0.

"1/x": [inverse transformation](https://en.wikipedia.org/wiki/Inverse_function). $1/x$ **(severe skew)**

"x\^2": [squared transformation](https://en.wikipedia.org/wiki/Quadratic_function). $x^2$

"x\^3": [cubed transformation](https://en.wikipedia.org/wiki/Cubic_function). $x^3$

We will compare the `sqrt`, `log+1`, `1/x` (inverse), `x^2`, and `x^3` transformations. Note that you would have to add a constant to use the `log` transformation, so it is easier to use the `log+1` instead. You however need to add a constant to both the `sqrt` and `1/x` transformations because they don't include zeros and will otherwise skew the results. *Here we removed zeros a priori*.

------------------------------------------------------------------------

#### Square-root Transformation

```{r sqrt-trans}
sqrtIns <- transform(InsMod$Insulin, method = "sqrt") 

summary(sqrtIns)
```

```{r sqrt-viz}
sqrtIns |>
  plot()
```

------------------------------------------------------------------------

#### Logarithmic (+1) Transformation

```{r log-trans}
Log1Ins <- transform(InsMod$Insulin, method = "log+1") 

summary(Log1Ins)
```

```{r log-viz}
Log1Ins |>
  plot()
```

------------------------------------------------------------------------

#### Inverse Transformation

```{r inv-trans}
InvIns <- transform(InsMod$Insulin, method = "1/x") 

summary(InvIns)
```

```{r inv-viz}
InvIns |>
  plot()
```

------------------------------------------------------------------------

#### Squared Transformation

```{r sqrd-trans}
SqrdIns <- transform(InsMod$Insulin, method = "x^2") 

summary(SqrdIns)
```

```{r sqrd-viz}
SqrdIns |>
  plot()
```

------------------------------------------------------------------------

#### Cubed Transformation

```{r cube-trans}
CubeIns <- transform(InsMod$Insulin, method = "x^3") 

summary(CubeIns)
```

```{r cube-viz}
CubeIns |>
  plot()
```

------------------------------------------------------------------------

### Box-cox Transformation

There are several transformations, each with it's own "criteria", and they don't always fix extremely skewed data. Instead, you can just choose the [Box-Cox transformation](https://en.wikipedia.org/wiki/Box%E2%80%93Cox_distribution) which searches for the the best lambda value that maximizes the log-likelihood (basically, what power transformation is best). The benefit is that you should have normally distributed data after, but the power relationship might be pretty abstract (i.e., what would a transformation of x\^0.12 be interpreted as in your system?..)

```{r boxcox-trans}
BoxCoxIns <- transform(InsMod$Insulin, method = "Box-Cox") 

summary(BoxCoxIns)
```

```{r boxcox-viz}
BoxCoxIns |>
  plot()
```

------------------------------------------------------------------------

## Produce an HTML Transformation Summary

```{r HTML-trans, eval = FALSE}
transformation_web_report(dataset)
```
