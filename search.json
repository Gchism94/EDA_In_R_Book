[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploratory Data Analysis In R Book",
    "section": "",
    "text": "Overview\nHere you will find a companion book for Exploratory Data Analysis in R Workshops prepared by the staff of the Data Science Institute\nThe links below take you to the GitHub repository for the associated workshop."
  },
  {
    "objectID": "index.html#diagnosing-like-a-data-doctor",
    "href": "index.html#diagnosing-like-a-data-doctor",
    "title": "Exploratory Data Analysis In R Book",
    "section": "Diagnosing like a data doctor",
    "text": "Diagnosing like a data doctor\n\nChapter summary\nExploring a novel data set and produce publication quality tables and reports"
  },
  {
    "objectID": "index.html#exploring-like-a-data-adventurer",
    "href": "index.html#exploring-like-a-data-adventurer",
    "title": "Exploratory Data Analysis In R Book",
    "section": "Exploring like a data adventurer",
    "text": "Exploring like a data adventurer\n\nChapter summary\nExploring the normality of numerical columns in a novel data set and producing publication quality tables and reports"
  },
  {
    "objectID": "index.html#transforming-like-a-data-transformer",
    "href": "index.html#transforming-like-a-data-transformer",
    "title": "Exploratory Data Analysis In R Book",
    "section": "Transforming like a Data… Transformer",
    "text": "Transforming like a Data… Transformer\n\nChapter summary\nUsing data transformation to correct non-normality in numerical data"
  },
  {
    "objectID": "index.html#imputing-like-a-data-scientist",
    "href": "index.html#imputing-like-a-data-scientist",
    "title": "Exploratory Data Analysis In R Book",
    "section": "Imputing like a Data Scientist",
    "text": "Imputing like a Data Scientist\n\nChapter summary\nExploring, visualizing, and imputing outliers and missing values (NAs) in a novel data set and produce publication quality graphs and tables"
  },
  {
    "objectID": "index.html#correlating-like-a-data-master",
    "href": "index.html#correlating-like-a-data-master",
    "title": "Exploratory Data Analysis In R Book",
    "section": "Correlating Like a Data Master",
    "text": "Correlating Like a Data Master\n\nChapter summary\nAssess relationships within a novel data set using publication quality tables and plots\n\nVisit our available Digital Learning Resources Library!\n\nCreated: 07/22/2022 (G. Chism); Last update: 09/14/2023\n CC BY-NC-SA"
  },
  {
    "objectID": "intro.html#what-is-exploratory-data-analysis",
    "href": "intro.html#what-is-exploratory-data-analysis",
    "title": "Introduction",
    "section": "What is Exploratory Data Analysis?",
    "text": "What is Exploratory Data Analysis?\nExploratory data analysis is a statistical, approach towards analyzing data sets to investigate and summarize their main characteristics, often through statistical graphics and other data visualization methods."
  },
  {
    "objectID": "intro.html#what-are-some-important-data-set-characteristics",
    "href": "intro.html#what-are-some-important-data-set-characteristics",
    "title": "Introduction",
    "section": "What are Some Important Data Set Characteristics?",
    "text": "What are Some Important Data Set Characteristics?\nThere are several characteristics that are arguably important, but we will only consider those covered in this workshop series. Let’s start with the fundamentals that will help guide us."
  },
  {
    "objectID": "intro.html#diagnostics",
    "href": "intro.html#diagnostics",
    "title": "Introduction",
    "section": "Diagnostics",
    "text": "Diagnostics\nWhen importing data sets, it is important to consider characteristics about the data columns, rows, and individual cells.\n\n\nVariables\nName of each variable\n\n\n\n\n\n\n\nPregnancies\n\n\nGlucose\n\n\nBloodPressure\n\n\nSkinThickness\n\n\nInsulin\n\n\nBMI\n\n\nDiabetesPedigreeFunction\n\n\nAge\n\n\nOutcome\n\n\nAge_group\n\n\n\n\n\n\n6\n\n\n148\n\n\n72\n\n\n35\n\n\n0\n\n\n33.6\n\n\n0.627\n\n\n50\n\n\n1\n\n\nMiddle\n\n\n\n\n1\n\n\n85\n\n\n66\n\n\n29\n\n\n0\n\n\n26.6\n\n\n0.351\n\n\n31\n\n\n0\n\n\nMiddle\n\n\n\n\n8\n\n\n183\n\n\n64\n\n\n0\n\n\n0\n\n\n23.3\n\n\n0.672\n\n\n32\n\n\n1\n\n\nMiddle\n\n\n\n\n1\n\n\n89\n\n\n66\n\n\n23\n\n\n94\n\n\n28.1\n\n\n0.167\n\n\n21\n\n\n0\n\n\nYoung\n\n\n\n\n0\n\n\n137\n\n\n40\n\n\n35\n\n\n168\n\n\n43.1\n\n\n2.288\n\n\n33\n\n\n1\n\n\nMiddle\n\n\n\n\n5\n\n\n116\n\n\n74\n\n\n0\n\n\n0\n\n\n25.6\n\n\n0.201\n\n\n30\n\n\n0\n\n\nYoung\n\n\n\n\n\n\n\n\n\nTypes\nData type of each variable\n\n\n\n\n\n\n\nvariables\n\n\ntypes\n\n\n\n\n\n\nPregnancies\n\n\ninteger\n\n\n\n\nGlucose\n\n\ninteger\n\n\n\n\nBloodPressure\n\n\ninteger\n\n\n\n\nSkinThickness\n\n\ninteger\n\n\n\n\nInsulin\n\n\ninteger\n\n\n\n\nBMI\n\n\nnumeric\n\n\n\n\nDiabetesPedigreeFunction\n\n\nnumeric\n\n\n\n\nAge\n\n\ninteger\n\n\n\n\nOutcome\n\n\ninteger\n\n\n\n\nAge_group\n\n\nfactor\n\n\n\n\n\n\n\n\nNumerical: Continuous\nMeasurable numbers that are fractional or decimal and cannot be counted (e.g., time, height, weight)\n\n\n\n\n\n\n\nNumerical: Discrete\nCountable whole numbers or integers (e.g., number of successes or failures)\n\n\n\n\n\n\n\n\nCategorical: Nominal\nLabeling variables without any order or quantitative value (e.g., hair color, nationality)\n\n\n\n\n\n\n\nCategorical: Ordinal\nWhere there is a hierarchical order along a scale (e.g., ranks, letter grades, age groups)\n\n\n\n\n\n\n\n\nMissing Values (NAs)\nCells, rows, or columns without data\n\nMissing percent: percentage of missing values * Unique count: number of unique values.\nUnique rate: rate of unique value - unique count / total number of observations.\n\n\n\n\n\n\n\n\nPregnancies\n\n\nGlucose\n\n\nBloodPressure\n\n\nSkinThickness\n\n\nInsulin\n\n\nBMI\n\n\nDiabetesPedigreeFunction\n\n\nAge\n\n\nOutcome\n\n\nAge_group\n\n\n\n\n\n\nNA\n\n\nNA\n\n\n72\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n0.627\n\n\n50\n\n\n1\n\n\nNA\n\n\n\n\n1\n\n\n85\n\n\n66\n\n\n29\n\n\n0\n\n\n26.6\n\n\n0.351\n\n\n31\n\n\n0\n\n\nMiddle\n\n\n\n\n8\n\n\n183\n\n\n64\n\n\n0\n\n\n0\n\n\n23.3\n\n\n0.672\n\n\n32\n\n\nNA\n\n\nMiddle\n\n\n\n\n1\n\n\nNA\n\n\nNA\n\n\n23\n\n\n94\n\n\n28.1\n\n\n0.167\n\n\nNA\n\n\n0\n\n\nYoung\n\n\n\n\n0\n\n\n137\n\n\n40\n\n\n35\n\n\n168\n\n\nNA\n\n\n2.288\n\n\n33\n\n\n1\n\n\nMiddle\n\n\n\n\n5\n\n\nNA\n\n\n74\n\n\nNA\n\n\n0\n\n\n25.6\n\n\n0.201\n\n\n30\n\n\n0\n\n\nNA"
  },
  {
    "objectID": "intro.html#summary-statistics",
    "href": "intro.html#summary-statistics",
    "title": "Introduction",
    "section": "Summary Statistics",
    "text": "Summary Statistics\nAbove we described some properties of data. However, you will need to know some descriptive characteristics of your data before you can move forward. Enter, summary statistics.\nSummary statistics allow you to summarize large amounts of information about your data as quickly as possible.\n\nCentral Tendency\nMeasuring a central property of your data. Some examples you’ve probably heard of are:\n\nMean: Average value\nMedian: Middle value\nMode: Most common value\n\n\n\n\n\n\nNotice however, that all values of central tendency can be pretty similar, such as in the top panel. This will become important when we discuss data transformations in Chapter 3.\n\n\nStatistical Dispersion\nMeasure of data variability, scatter, or spread. Some examples you may have heard of:\n\nStandard deviation (SD): The amount of variation that occurs in a set of values.\nInterquartile range (IQR): The difference between the 75th and 25th percentiles\nOutliers: A value outside of \\(1.5 * IQR\\)\n\n\n\n\n\n\n\n\nDistribution Shape\nMeasures of describing the shape of a distribution, usually compared to a normal distribution (bell-curve)\n\nSkewness: The symmetry of the distribution\nKurtosis: The tailedness of the distribution\n\n\n\n\n\n\n\n\nStatistical Dependence (Correlation)\nMeasure of causality between two random variables (statistically). Notably, we approximate causality with correlations (see correlation \\(\\neq\\) causation)\n\nNumerical values, but you can compare numericals across categories (see the first plot above)."
  },
  {
    "objectID": "DiagnosingLikeDataDoctor.html#purpose-of-this-chapter",
    "href": "DiagnosingLikeDataDoctor.html#purpose-of-this-chapter",
    "title": "Diagnosing like a Data Doctor",
    "section": "Purpose of this chapter",
    "text": "Purpose of this chapter\nExploring a novel data set and produce publication quality tables and reports"
  },
  {
    "objectID": "DiagnosingLikeDataDoctor.html#take-aways",
    "href": "DiagnosingLikeDataDoctor.html#take-aways",
    "title": "Diagnosing like a Data Doctor",
    "section": "Take-aways",
    "text": "Take-aways\n\nLoad and explore a data set with publication quality tables\nDiagnose outliers and missing values in a data set\nPrepare an HTML summary report showcasing properties of a data set"
  },
  {
    "objectID": "DiagnosingLikeDataDoctor.html#required-setup",
    "href": "DiagnosingLikeDataDoctor.html#required-setup",
    "title": "Diagnosing like a Data Doctor",
    "section": "Required Setup",
    "text": "Required Setup\nWe first need to prepare our environment with the necessary packages\n\n# Sets the repository to download packages from\noptions(repos = list(CRAN = \"http://cran.rstudio.com/\"))\n\n# Sets the number of significant figures to two - e.g., 0.01\noptions(digits = 2)\n\n# Required package for quick package downloading and loading \ninstall.packages(\"pacman\")\n\n# Downloads and load required packages\npacman::p_load(dlookr, # Exploratory data analysis\n               formattable, # HTML tables from R outputs\n               here, # Standardizes paths to data\n               kableExtra, # Alternative to formattable\n               knitr, # Needed to write HTML reports\n               missRanger, # To generate NAs\n               tidyverse) # Powerful data wrangling package suite"
  },
  {
    "objectID": "DiagnosingLikeDataDoctor.html#load-and-examine-a-data-set",
    "href": "DiagnosingLikeDataDoctor.html#load-and-examine-a-data-set",
    "title": "Diagnosing like a Data Doctor",
    "section": "Load and Examine a Data Set",
    "text": "Load and Examine a Data Set\n\nLoad data and view\nExamine columns and data types\nDefine box plots\nDescribe meta data\n\nWe will be using open source data from UArizona researchers for Test, Trace, Treat (T3) efforts offers two clinical diagnostic tests (Antigen, RT-PCR) to determine whether an individual is currently infected with the COVID-19 virus. (Merchant et al. 2022)\n\n# Let's load a data set from the COVID-19 daily testing data set\ndataset &lt;- read.csv(here(\"EDA_In_R_Book\", \"data\", \"daily_summary.csv\")) \n\n# What does the data look like?\ndataset |&gt;\n  head() |&gt;\n  formattable()\n\n\n\n\n\n\nresult_date\n\n\naffil_category\n\n\ntest_type\n\n\ntest_result\n\n\ntest_count\n\n\ntest_source\n\n\n\n\n\n\n2020-08-04\n\n\nEmployee\n\n\nAntigen\n\n\nNegative\n\n\n5\n\n\nCampus Health\n\n\n\n\n2020-08-04\n\n\nEmployee\n\n\nAntigen\n\n\nPositive\n\n\n0\n\n\nCampus Health\n\n\n\n\n2020-08-04\n\n\nEmployee\n\n\nAntigen\n\n\nNegative\n\n\n1\n\n\nTest All Test Smart\n\n\n\n\n2020-08-04\n\n\nEmployee\n\n\nAntigen\n\n\nPositive\n\n\n0\n\n\nTest All Test Smart\n\n\n\n\n2020-08-04\n\n\nOff-Campus Student\n\n\nAntigen\n\n\nNegative\n\n\n9\n\n\nCampus Health\n\n\n\n\n2020-08-04\n\n\nOff-Campus Student\n\n\nAntigen\n\n\nPositive\n\n\n1\n\n\nCampus Health"
  },
  {
    "objectID": "DiagnosingLikeDataDoctor.html#diagnose-your-data",
    "href": "DiagnosingLikeDataDoctor.html#diagnose-your-data",
    "title": "Diagnosing like a Data Doctor",
    "section": "Diagnose your Data",
    "text": "Diagnose your Data\n\n# What are the properties of the data\ndataset |&gt;\n  diagnose() |&gt;\n  formattable()\n\n\n\n\n\n\nvariables\n\n\ntypes\n\n\nmissing_count\n\n\nmissing_percent\n\n\nunique_count\n\n\nunique_rate\n\n\n\n\n\n\nresult_date\n\n\ncharacter\n\n\n0\n\n\n0\n\n\n541\n\n\n0.05893\n\n\n\n\naffil_category\n\n\ncharacter\n\n\n0\n\n\n0\n\n\n4\n\n\n0.00044\n\n\n\n\ntest_type\n\n\ncharacter\n\n\n0\n\n\n0\n\n\n3\n\n\n0.00033\n\n\n\n\ntest_result\n\n\ncharacter\n\n\n0\n\n\n0\n\n\n3\n\n\n0.00033\n\n\n\n\ntest_count\n\n\ninteger\n\n\n0\n\n\n0\n\n\n591\n\n\n0.06438\n\n\n\n\ntest_source\n\n\ncharacter\n\n\n0\n\n\n0\n\n\n2\n\n\n0.00022\n\n\n\n\n\n\n\n\nvariables: name of each variable\ntypes: data type of each variable\nmissing_count: number of missing values\nmissing_percent: percentage of missing values\nunique_count: number of unique values\nunique_rate: rate of unique value - unique_count / number of observations"
  },
  {
    "objectID": "DiagnosingLikeDataDoctor.html#summary-statistics-of-your-data",
    "href": "DiagnosingLikeDataDoctor.html#summary-statistics-of-your-data",
    "title": "Diagnosing like a Data Doctor",
    "section": "Summary Statistics of your Data",
    "text": "Summary Statistics of your Data\n\nNumerical Variables\n\n# Summary statistics of our numerical columns\ndataset |&gt;\n  diagnose_numeric() |&gt;\n  formattable()\n\n\n\n\n\n\nvariables\n\n\nmin\n\n\nQ1\n\n\nmean\n\n\nmedian\n\n\nQ3\n\n\nmax\n\n\nzero\n\n\nminus\n\n\noutlier\n\n\n\n\n\n\ntest_count\n\n\n0\n\n\n0\n\n\n47\n\n\n2\n\n\n16\n\n\n1472\n\n\n2777\n\n\n0\n\n\n1721\n\n\n\n\n\n\n\n\nmin: minimum value\nQ1: 1/4 quartile, 25th percentile\nmean: arithmetic mean (average value)\nmedian: median, 50th percentile\nQ3: 3/4 quartile, 75th percentile\nmax: maximum value\nzero: number of observations with the value 0\nminus: number of observations with negative numbers\noutlier: number of outliers\n\n\n\n\nOutliers\nValues outside of \\(1.5 * IQR\\)\n\n\n\nImage Credit: CÉDRIC SCHERER\n\n\n\nThere are several numerical variables that have outliers above, let’s see what the data look like with and without them\n\nCreate a table with columns containing outliers\nPlot outliers in a box plot and histogram\n\n\n# Table showing outliers\ndiagnose_outlier(dataset) |&gt;\n  filter(outliers_ratio &gt; 0) |&gt;\n  formattable()\n\n\n\n\n\n\nvariables\n\n\noutliers_cnt\n\n\noutliers_ratio\n\n\noutliers_mean\n\n\nwith_mean\n\n\nwithout_mean\n\n\n\n\n\n\ntest_count\n\n\n1721\n\n\n19\n\n\n231\n\n\n47\n\n\n4.3\n\n\n\n\n\n\n\n\noutliers_cnt: number of outliers\noutliers_ratio: ratio of outliers over all values\noutliers_mean: arithmetic mean (average value) of outlier values\nwith_mean: arithmetic mean of all values including outliers\nwithout_mean: arithmetic mean of all values excluding outliers\n\n\n# Selecting desired columns \ndataset |&gt;\n    plot_outlier()\n\n\n\n\n\n\n\nMissing Values (NAs)\n\nTable showing the extent of NAs in columns containing them\nPlot showing the frequency of missing values\n\n\n# Create the NA table\ndataset |&gt;\n  generateNA(p = 0.3) |&gt;\n  plot_na_pareto(only_na = TRUE, plot = FALSE) |&gt;\n  formattable() # Publishable table\n\n\n\n\n\n\nvariable\n\n\nfrequencies\n\n\nratio\n\n\ngrade\n\n\ncumulative\n\n\n\n\n\n\naffil_category\n\n\n2754\n\n\n0.3\n\n\nBad\n\n\n17\n\n\n\n\nresult_date\n\n\n2754\n\n\n0.3\n\n\nBad\n\n\n33\n\n\n\n\ntest_count\n\n\n2754\n\n\n0.3\n\n\nBad\n\n\n50\n\n\n\n\ntest_result\n\n\n2754\n\n\n0.3\n\n\nBad\n\n\n67\n\n\n\n\ntest_source\n\n\n2754\n\n\n0.3\n\n\nBad\n\n\n83\n\n\n\n\ntest_type\n\n\n2754\n\n\n0.3\n\n\nBad\n\n\n100\n\n\n\n\n\n\n\n\n# Plot the intersect of the columns with the most missing values\n# This means that some combinations of columns have missing values in the same row\ndataset |&gt;\n  generateNA(p = 0.3) |&gt;\n  select(test_type, test_result, test_count) |&gt;\n  plot_na_intersect(only_na = TRUE) \n\n\n\n\n\n\n\nCategorical Variables\n\n# Diagnose our categorical columns\ndataset |&gt;\n  diagnose_category() |&gt; \n  formattable()\n\n\n\n\n\n\nvariables\n\n\nlevels\n\n\nN\n\n\nfreq\n\n\nratio\n\n\nrank\n\n\n\n\n\n\nresult_date\n\n\n2020-09-17\n\n\n9180\n\n\n26\n\n\n0.283\n\n\n1\n\n\n\n\nresult_date\n\n\n2020-09-23\n\n\n9180\n\n\n26\n\n\n0.283\n\n\n1\n\n\n\n\nresult_date\n\n\n2020-10-01\n\n\n9180\n\n\n26\n\n\n0.283\n\n\n1\n\n\n\n\nresult_date\n\n\n2020-10-08\n\n\n9180\n\n\n26\n\n\n0.283\n\n\n1\n\n\n\n\nresult_date\n\n\n2020-09-01\n\n\n9180\n\n\n25\n\n\n0.272\n\n\n5\n\n\n\n\nresult_date\n\n\n2020-09-16\n\n\n9180\n\n\n25\n\n\n0.272\n\n\n5\n\n\n\n\nresult_date\n\n\n2020-09-24\n\n\n9180\n\n\n25\n\n\n0.272\n\n\n5\n\n\n\n\nresult_date\n\n\n2020-12-09\n\n\n9180\n\n\n25\n\n\n0.272\n\n\n5\n\n\n\n\nresult_date\n\n\n2020-12-15\n\n\n9180\n\n\n25\n\n\n0.272\n\n\n5\n\n\n\n\nresult_date\n\n\n2020-09-04\n\n\n9180\n\n\n24\n\n\n0.261\n\n\n10\n\n\n\n\naffil_category\n\n\nOff-Campus Student\n\n\n9180\n\n\n3368\n\n\n36.688\n\n\n1\n\n\n\n\naffil_category\n\n\nEmployee\n\n\n9180\n\n\n2987\n\n\n32.538\n\n\n2\n\n\n\n\naffil_category\n\n\nOn-Campus Student\n\n\n9180\n\n\n2823\n\n\n30.752\n\n\n3\n\n\n\n\naffil_category\n\n\nOther\n\n\n9180\n\n\n2\n\n\n0.022\n\n\n4\n\n\n\n\ntest_type\n\n\nAntigen\n\n\n9180\n\n\n4624\n\n\n50.370\n\n\n1\n\n\n\n\ntest_type\n\n\nPCR\n\n\n9180\n\n\n4554\n\n\n49.608\n\n\n2\n\n\n\n\ntest_type\n\n\nAntibody\n\n\n9180\n\n\n2\n\n\n0.022\n\n\n3\n\n\n\n\ntest_result\n\n\nNegative\n\n\n9180\n\n\n4575\n\n\n49.837\n\n\n1\n\n\n\n\ntest_result\n\n\nPositive\n\n\n9180\n\n\n4575\n\n\n49.837\n\n\n1\n\n\n\n\ntest_result\n\n\nInconclusive\n\n\n9180\n\n\n30\n\n\n0.327\n\n\n3\n\n\n\n\ntest_source\n\n\nTest All Test Smart\n\n\n9180\n\n\n5078\n\n\n55.316\n\n\n1\n\n\n\n\ntest_source\n\n\nCampus Health\n\n\n9180\n\n\n4102\n\n\n44.684\n\n\n2\n\n\n\n\n\n\n\n\nvariables: category names\nlevels: group names within categories\nN: number of observation\nfreq: number of observation at group level / number of observation at category level\nratio: percentage of observation at group level / number of observation at category level\nrank: rank of the occupancy ratio of levels (order in which the groups are in the category)"
  },
  {
    "objectID": "DiagnosingLikeDataDoctor.html#produce-an-html-summary-of-a-data-set",
    "href": "DiagnosingLikeDataDoctor.html#produce-an-html-summary-of-a-data-set",
    "title": "Diagnosing like a Data Doctor",
    "section": "Produce an HTML Summary of a Data Set",
    "text": "Produce an HTML Summary of a Data Set\n\ndiagnose_web_report(dataset)\n\n\n\n\n\nMerchant, Nirav C, Jim Davis, George H Franks, Chun Ly, Fernando Rios, Todd Wickizer, Gary D Windham, and Michelle Yung. 2022. “University of Arizona Test-Trace-Treat COVID-19 Testing Results.” University of Arizona Research Data Repository. https://doi.org/10.25422/AZU.DATA.14869740.V3."
  },
  {
    "objectID": "ExploringLikeDataAdventurer.html#purpose-of-this-chapter",
    "href": "ExploringLikeDataAdventurer.html#purpose-of-this-chapter",
    "title": "Exploring like a Data Adventurer",
    "section": "Purpose of this chapter",
    "text": "Purpose of this chapter\nExploring the normality of numerical columns in a novel data set and producing publication quality tables and reports"
  },
  {
    "objectID": "ExploringLikeDataAdventurer.html#take-aways",
    "href": "ExploringLikeDataAdventurer.html#take-aways",
    "title": "Exploring like a Data Adventurer",
    "section": "Take-aways",
    "text": "Take-aways\n\nUsing summary statistics to better understand individual columns in a data set.\nAssessing data normality in numerical columns.\nProducing a publishable HTML with summary statistics and normality tests for columns within a data set."
  },
  {
    "objectID": "ExploringLikeDataAdventurer.html#required-setup",
    "href": "ExploringLikeDataAdventurer.html#required-setup",
    "title": "Exploring like a Data Adventurer",
    "section": "Required Setup",
    "text": "Required Setup\nWe first need to prepare our environment with the necessary packages\n\n# Sets the repository to download packages from\noptions(repos = list(CRAN = \"http://cran.rstudio.com/\"))\n\n# Sets the number of significant figures to two - e.g., 0.01\noptions(digits = 2)\n\n# Required package for quick package downloading and loading \ninstall.packages(\"pacman\")\n\npacman::p_load(dlookr, # Exploratory data analysis\n               formattable, # HTML tables from R outputs\n               here, # Standardizes paths to data\n               kableExtra, # Alternative to formattable\n               knitr, # Needed to write HTML reports\n               tidyverse) # Powerful data wrangling package suite"
  },
  {
    "objectID": "ExploringLikeDataAdventurer.html#load-and-examine-a-data-set",
    "href": "ExploringLikeDataAdventurer.html#load-and-examine-a-data-set",
    "title": "Exploring like a Data Adventurer",
    "section": "Load and Examine a Data Set",
    "text": "Load and Examine a Data Set\nWe will be using open source data from UArizona researchers that investigates the effects of climate change on canopy trees. (Meredith, Ladd, and Werner 2021)\n\n# Let's load a data set from the canopy tree data set\ndataset &lt;- read.csv(here(\"EDA_In_R_Book\", \"data\", \"Data_Fig2_Repo.csv\")) \n\n# What does the data look like?\ndataset |&gt;\n  head() |&gt;\n  formattable()\n\n\n\n\n\n\nDate\n\n\nGroup\n\n\nSap_Flow\n\n\nTWaterFlux\n\n\npLWP\n\n\nmLWP\n\n\n\n\n\n\n10/4/19\n\n\nDrought-sens-canopy\n\n\n184.0\n\n\n82.2\n\n\n-0.26\n\n\n-0.68\n\n\n\n\n10/4/19\n\n\nDrought-sens-under\n\n\n2.5\n\n\n1.3\n\n\n-0.30\n\n\n-0.76\n\n\n\n\n10/4/19\n\n\nDrought-tol-canopy\n\n\n10.6\n\n\n4.4\n\n\n-0.44\n\n\n-0.72\n\n\n\n\n10/4/19\n\n\nDrought-tol-under\n\n\n4.4\n\n\n2.1\n\n\n-0.21\n\n\n-0.70\n\n\n\n\n10/5/19\n\n\nDrought-sens-canopy\n\n\n182.9\n\n\n95.9\n\n\n-0.28\n\n\n-0.71\n\n\n\n\n10/5/19\n\n\nDrought-sens-under\n\n\n2.5\n\n\n1.2\n\n\n-0.32\n\n\n-0.79"
  },
  {
    "objectID": "ExploringLikeDataAdventurer.html#diagnose-your-data",
    "href": "ExploringLikeDataAdventurer.html#diagnose-your-data",
    "title": "Exploring like a Data Adventurer",
    "section": "Diagnose your Data",
    "text": "Diagnose your Data\n\n# What are the properties of the data\ndataset |&gt;\n  diagnose() |&gt;\n  formattable()\n\n\n\n\n\n\nvariables\n\n\ntypes\n\n\nmissing_count\n\n\nmissing_percent\n\n\nunique_count\n\n\nunique_rate\n\n\n\n\n\n\nDate\n\n\ncharacter\n\n\n0\n\n\n0\n\n\n147\n\n\n0.2500\n\n\n\n\nGroup\n\n\ncharacter\n\n\n0\n\n\n0\n\n\n4\n\n\n0.0068\n\n\n\n\nSap_Flow\n\n\nnumeric\n\n\n108\n\n\n18\n\n\n481\n\n\n0.8180\n\n\n\n\nTWaterFlux\n\n\nnumeric\n\n\n0\n\n\n0\n\n\n508\n\n\n0.8639\n\n\n\n\npLWP\n\n\nnumeric\n\n\n312\n\n\n53\n\n\n277\n\n\n0.4711\n\n\n\n\nmLWP\n\n\nnumeric\n\n\n280\n\n\n48\n\n\n309\n\n\n0.5255\n\n\n\n\n\n\n\n\nvariables: name of each variable\ntypes: data type of each variable\nmissing_count: number of missing values\nmissing_percent: percentage of missing values\nunique_count: number of unique values\nunique_rate: rate of unique value - unique_count / number of observations\n\n\n\nBox Plot\n\n\n\nImage Credit: CÉDRIC SCHERER\n\n\n\n\n\nSkewness\n\n\n\n(c) Andrey Akinshin\n\n\n\n\nNOTE\n\n“Skewness” has multiple definitions. Several underlying equations mey be at play\nSkewness is “designed” for distributions with one peak (unimodal); it’s meaningless for distributions with multiple peaks (multimodal).\nMost default skewness definitions are not robust: a single outlier could completely distort the skewness value.\nWe can’t make conclusions about the locations of the mean and the median based on the skewness sign.\n\n\n\n\n\nKurtosis\n\n\n\n(c) Andrey Akinshin\n\n\n\n\nNOTE\n\nThere are multiple definitions of kurtosis - i.e., “kurtosis” and “excess kurtosis,” but there are other definitions of this measure.\nKurtosis may work fine for distributions with one peak (unimodal); it’s meaningless for distributions with multiple peaks (multimodal).\nThe classic definition of kurtosis is not robust: it could be easily spoiled by extreme outliers."
  },
  {
    "objectID": "ExploringLikeDataAdventurer.html#describe-your-continuous-data",
    "href": "ExploringLikeDataAdventurer.html#describe-your-continuous-data",
    "title": "Exploring like a Data Adventurer",
    "section": "Describe your Continuous Data",
    "text": "Describe your Continuous Data\n\n# Summary statistics \ndataset |&gt;\n  describe() |&gt;\n  formattable()\n\n\n\n\n\n\ndescribed_variables\n\n\nn\n\n\nna\n\n\nmean\n\n\nsd\n\n\nse_mean\n\n\nIQR\n\n\nskewness\n\n\nkurtosis\n\n\np00\n\n\np01\n\n\np05\n\n\np10\n\n\np20\n\n\np25\n\n\np30\n\n\np40\n\n\np50\n\n\np60\n\n\np70\n\n\np75\n\n\np80\n\n\np90\n\n\np95\n\n\np99\n\n\np100\n\n\n\n\n\n\nSap_Flow\n\n\n480\n\n\n108\n\n\n25.09\n\n\n40.52\n\n\n1.849\n\n\n13.92\n\n\n2.2\n\n\n4.20\n\n\n0.17\n\n\n0.33\n\n\n0.47\n\n\n1.15\n\n\n2.26\n\n\n2.45\n\n\n3.76\n\n\n5.05\n\n\n5.82\n\n\n9.03\n\n\n10.51\n\n\n16.37\n\n\n48.89\n\n\n83.48\n\n\n109.84\n\n\n176.99\n\n\n184.04\n\n\n\n\nTWaterFlux\n\n\n588\n\n\n0\n\n\n11.93\n\n\n19.05\n\n\n0.786\n\n\n6.28\n\n\n2.1\n\n\n3.88\n\n\n0.10\n\n\n0.15\n\n\n0.22\n\n\n0.60\n\n\n1.14\n\n\n1.29\n\n\n1.70\n\n\n2.31\n\n\n3.00\n\n\n4.20\n\n\n5.21\n\n\n7.58\n\n\n23.37\n\n\n43.14\n\n\n51.81\n\n\n80.87\n\n\n96.01\n\n\n\n\npLWP\n\n\n276\n\n\n312\n\n\n-0.61\n\n\n0.23\n\n\n0.014\n\n\n0.26\n\n\n-1.1\n\n\n1.77\n\n\n-1.43\n\n\n-1.32\n\n\n-1.07\n\n\n-0.88\n\n\n-0.73\n\n\n-0.71\n\n\n-0.68\n\n\n-0.62\n\n\n-0.59\n\n\n-0.55\n\n\n-0.49\n\n\n-0.45\n\n\n-0.41\n\n\n-0.35\n\n\n-0.30\n\n\n-0.24\n\n\n-0.21\n\n\n\n\nmLWP\n\n\n308\n\n\n280\n\n\n-1.03\n\n\n0.30\n\n\n0.017\n\n\n0.42\n\n\n-0.8\n\n\n-0.18\n\n\n-1.81\n\n\n-1.79\n\n\n-1.62\n\n\n-1.46\n\n\n-1.32\n\n\n-1.23\n\n\n-1.13\n\n\n-1.04\n\n\n-0.95\n\n\n-0.90\n\n\n-0.84\n\n\n-0.81\n\n\n-0.76\n\n\n-0.71\n\n\n-0.67\n\n\n-0.59\n\n\n-0.55\n\n\n\n\n\n\n\n\ndescribes_variables: name of the column being described\nn: number of observations excluding missing values\nna: number of missing values\nmean: arithmetic average\nsd: standard deviation\nse_mean: standard error mean. sd/sqrt(n)\nIQR: interquartile range (Q3-Q1)\nskewness: skewness\nkurtosis: kurtosis\np25: Q1. 25% percentile\np50: median. 50% percentile\np75: Q3. 75% percentile\np01, p05, p10, p20, p30: 1%, 5%, 20%, 30% percentiles\np40, p60, p70, p80: 40%, 60%, 70%, 80% percentiles\np90, p95, p99, p100: 90%, 95%, 99%, 100% percentiles\n\n\n\nDescribe your Continuous Data: Refined\nThe above is pretty overwhelming, and most people don’t care about percentiles outside of Q1, Q3, and the median (Q2).\n\n# Summary statistics, selecting the desired ones\ndataset |&gt;\n  describe() |&gt;\n  select(described_variables, n, na, mean, sd, se_mean, IQR, skewness, kurtosis, p25, p50, p75) |&gt;\n  formattable()\n\n\n\n\n\n\ndescribed_variables\n\n\nn\n\n\nna\n\n\nmean\n\n\nsd\n\n\nse_mean\n\n\nIQR\n\n\nskewness\n\n\nkurtosis\n\n\np25\n\n\np50\n\n\np75\n\n\n\n\n\n\nSap_Flow\n\n\n480\n\n\n108\n\n\n25.09\n\n\n40.52\n\n\n1.849\n\n\n13.92\n\n\n2.2\n\n\n4.20\n\n\n2.45\n\n\n5.82\n\n\n16.37\n\n\n\n\nTWaterFlux\n\n\n588\n\n\n0\n\n\n11.93\n\n\n19.05\n\n\n0.786\n\n\n6.28\n\n\n2.1\n\n\n3.88\n\n\n1.29\n\n\n3.00\n\n\n7.58\n\n\n\n\npLWP\n\n\n276\n\n\n312\n\n\n-0.61\n\n\n0.23\n\n\n0.014\n\n\n0.26\n\n\n-1.1\n\n\n1.77\n\n\n-0.71\n\n\n-0.59\n\n\n-0.45\n\n\n\n\nmLWP\n\n\n308\n\n\n280\n\n\n-1.03\n\n\n0.30\n\n\n0.017\n\n\n0.42\n\n\n-0.8\n\n\n-0.18\n\n\n-1.23\n\n\n-0.95\n\n\n-0.81"
  },
  {
    "objectID": "ExploringLikeDataAdventurer.html#describe-categorical-variables",
    "href": "ExploringLikeDataAdventurer.html#describe-categorical-variables",
    "title": "Exploring like a Data Adventurer",
    "section": "Describe Categorical Variables",
    "text": "Describe Categorical Variables\n\ndataset |&gt;\n  diagnose_category() |&gt;\n  formattable()\n\n\n\n\n\n\nvariables\n\n\nlevels\n\n\nN\n\n\nfreq\n\n\nratio\n\n\nrank\n\n\n\n\n\n\nDate\n\n\n1/1/20\n\n\n588\n\n\n4\n\n\n0.68\n\n\n1\n\n\n\n\nDate\n\n\n1/10/20\n\n\n588\n\n\n4\n\n\n0.68\n\n\n1\n\n\n\n\nDate\n\n\n1/11/20\n\n\n588\n\n\n4\n\n\n0.68\n\n\n1\n\n\n\n\nDate\n\n\n1/12/20\n\n\n588\n\n\n4\n\n\n0.68\n\n\n1\n\n\n\n\nDate\n\n\n1/13/20\n\n\n588\n\n\n4\n\n\n0.68\n\n\n1\n\n\n\n\nDate\n\n\n1/14/20\n\n\n588\n\n\n4\n\n\n0.68\n\n\n1\n\n\n\n\nDate\n\n\n1/15/20\n\n\n588\n\n\n4\n\n\n0.68\n\n\n1\n\n\n\n\nDate\n\n\n1/16/20\n\n\n588\n\n\n4\n\n\n0.68\n\n\n1\n\n\n\n\nDate\n\n\n1/17/20\n\n\n588\n\n\n4\n\n\n0.68\n\n\n1\n\n\n\n\nDate\n\n\n1/18/20\n\n\n588\n\n\n4\n\n\n0.68\n\n\n1\n\n\n\n\nGroup\n\n\nDrought-sens-canopy\n\n\n588\n\n\n147\n\n\n25.00\n\n\n1\n\n\n\n\nGroup\n\n\nDrought-sens-under\n\n\n588\n\n\n147\n\n\n25.00\n\n\n1\n\n\n\n\nGroup\n\n\nDrought-tol-canopy\n\n\n588\n\n\n147\n\n\n25.00\n\n\n1\n\n\n\n\nGroup\n\n\nDrought-tol-under\n\n\n588\n\n\n147\n\n\n25.00\n\n\n1\n\n\n\n\n\n\n\n\nvariables: category names\nlevels: group names within categories\nN: number of observation\nfreq: number of observation at group level / number of observation at category level\nratio: percentage of observation at group level / number of observation at category level\nrank: rank of the occupancy ratio of levels (order in which the groups are in the category)\n\n\n\nGroup Descriptive Statistics\n\ndataset |&gt;\n  group_by(Group) |&gt;\n  describe() |&gt;\n  select(described_variables, Group, n, na, mean, sd, se_mean, IQR, skewness, kurtosis, p25, p50, p75) |&gt;\n  formattable()\n\n\n\n\n\n\ndescribed_variables\n\n\nGroup\n\n\nn\n\n\nna\n\n\nmean\n\n\nsd\n\n\nse_mean\n\n\nIQR\n\n\nskewness\n\n\nkurtosis\n\n\np25\n\n\np50\n\n\np75\n\n\n\n\n\n\nSap_Flow\n\n\nDrought-sens-canopy\n\n\n120\n\n\n27\n\n\n85.27\n\n\n41.314\n\n\n3.771\n\n\n40.09\n\n\n1.0493\n\n\n0.150\n\n\n53.98\n\n\n76.72\n\n\n94.07\n\n\n\n\nSap_Flow\n\n\nDrought-sens-under\n\n\n120\n\n\n27\n\n\n1.45\n\n\n0.804\n\n\n0.073\n\n\n1.66\n\n\n-0.2242\n\n\n-1.635\n\n\n0.53\n\n\n1.67\n\n\n2.19\n\n\n\n\nSap_Flow\n\n\nDrought-tol-canopy\n\n\n120\n\n\n27\n\n\n9.07\n\n\n1.396\n\n\n0.127\n\n\n2.28\n\n\n-0.6344\n\n\n-0.695\n\n\n8.12\n\n\n9.29\n\n\n10.40\n\n\n\n\nSap_Flow\n\n\nDrought-tol-under\n\n\n120\n\n\n27\n\n\n4.57\n\n\n0.902\n\n\n0.082\n\n\n1.09\n\n\n-0.9141\n\n\n-0.077\n\n\n4.05\n\n\n4.94\n\n\n5.14\n\n\n\n\nTWaterFlux\n\n\nDrought-sens-canopy\n\n\n147\n\n\n0\n\n\n40.40\n\n\n19.028\n\n\n1.569\n\n\n24.88\n\n\n0.9210\n\n\n0.509\n\n\n25.22\n\n\n38.63\n\n\n50.10\n\n\n\n\nTWaterFlux\n\n\nDrought-sens-under\n\n\n147\n\n\n0\n\n\n0.75\n\n\n0.429\n\n\n0.035\n\n\n0.84\n\n\n0.0105\n\n\n-1.188\n\n\n0.27\n\n\n0.82\n\n\n1.11\n\n\n\n\nTWaterFlux\n\n\nDrought-tol-canopy\n\n\n147\n\n\n0\n\n\n4.36\n\n\n0.940\n\n\n0.078\n\n\n1.51\n\n\n-0.3548\n\n\n-0.940\n\n\n3.60\n\n\n4.46\n\n\n5.11\n\n\n\n\nTWaterFlux\n\n\nDrought-tol-under\n\n\n147\n\n\n0\n\n\n2.19\n\n\n0.598\n\n\n0.049\n\n\n0.95\n\n\n-0.0087\n\n\n-0.779\n\n\n1.74\n\n\n2.20\n\n\n2.69\n\n\n\n\nmLWP\n\n\nDrought-sens-canopy\n\n\n77\n\n\n70\n\n\n-1.32\n\n\n0.298\n\n\n0.034\n\n\n0.41\n\n\n0.3178\n\n\n-0.691\n\n\n-1.53\n\n\n-1.35\n\n\n-1.11\n\n\n\n\nmLWP\n\n\nDrought-sens-under\n\n\n77\n\n\n70\n\n\n-1.10\n\n\n0.264\n\n\n0.030\n\n\n0.43\n\n\n-0.3411\n\n\n-0.313\n\n\n-1.34\n\n\n-1.05\n\n\n-0.91\n\n\n\n\nmLWP\n\n\nDrought-tol-canopy\n\n\n77\n\n\n70\n\n\n-0.89\n\n\n0.092\n\n\n0.010\n\n\n0.12\n\n\n-0.2608\n\n\n-0.402\n\n\n-0.95\n\n\n-0.89\n\n\n-0.83\n\n\n\n\nmLWP\n\n\nDrought-tol-under\n\n\n77\n\n\n70\n\n\n-0.81\n\n\n0.171\n\n\n0.019\n\n\n0.21\n\n\n-0.9539\n\n\n-0.412\n\n\n-0.91\n\n\n-0.74\n\n\n-0.70\n\n\n\n\npLWP\n\n\nDrought-sens-canopy\n\n\n69\n\n\n78\n\n\n-0.67\n\n\n0.246\n\n\n0.030\n\n\n0.32\n\n\n-0.3510\n\n\n-0.274\n\n\n-0.79\n\n\n-0.71\n\n\n-0.47\n\n\n\n\npLWP\n\n\nDrought-sens-under\n\n\n69\n\n\n78\n\n\n-0.70\n\n\n0.284\n\n\n0.034\n\n\n0.28\n\n\n-1.1510\n\n\n0.480\n\n\n-0.80\n\n\n-0.59\n\n\n-0.52\n\n\n\n\npLWP\n\n\nDrought-tol-canopy\n\n\n69\n\n\n78\n\n\n-0.63\n\n\n0.096\n\n\n0.012\n\n\n0.14\n\n\n-0.4644\n\n\n-0.592\n\n\n-0.71\n\n\n-0.60\n\n\n-0.57\n\n\n\n\npLWP\n\n\nDrought-tol-under\n\n\n69\n\n\n78\n\n\n-0.44\n\n\n0.132\n\n\n0.016\n\n\n0.16\n\n\n-0.4333\n\n\n-0.432\n\n\n-0.52\n\n\n-0.41\n\n\n-0.36"
  },
  {
    "objectID": "ExploringLikeDataAdventurer.html#testing-normality",
    "href": "ExploringLikeDataAdventurer.html#testing-normality",
    "title": "Exploring like a Data Adventurer",
    "section": "Testing Normality",
    "text": "Testing Normality\n\nShapiro-Wilk test & Q-Q plots\nTesting overall normality of two columns\nTesting normality of groups\n\n\n\nNormality of Columns\n\n\nShapiro-Wilk Test\nShapiro-Wilk test looks at whether a target distribution is sample form a normal distribution\n\ndataset |&gt;\n  normality() |&gt;\n  formattable()\n\n\n\n\n\n\nvars\n\n\nstatistic\n\n\np_value\n\n\nsample\n\n\n\n\n\n\nSap_Flow\n\n\n0.63\n\n\n7.4e-31\n\n\n588\n\n\n\n\nTWaterFlux\n\n\n0.64\n\n\n2.2e-33\n\n\n588\n\n\n\n\npLWP\n\n\n0.93\n\n\n3.4e-10\n\n\n588\n\n\n\n\nmLWP\n\n\n0.93\n\n\n7.0e-11\n\n\n588\n\n\n\n\n\n\n\n\n\n\nQ-Q Plots\nPlots of the quartiles of a target data set and plot it against predicted quartiles from a normal distribution\n\ndataset |&gt;\nplot_normality()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNormality within Groups\nLooking within Age_group at the subgroup normality\n\nShapiro-Wilk Test\n\ndataset |&gt;\n  group_by(Group) |&gt;\n  select(Sap_Flow, TWaterFlux, Group) |&gt;\n  normality() |&gt;\n  formattable()\n\n\n\n\n\n\nvariable\n\n\nGroup\n\n\nstatistic\n\n\np_value\n\n\nsample\n\n\n\n\n\n\nSap_Flow\n\n\nDrought-sens-canopy\n\n\n0.87\n\n\n9.8e-09\n\n\n147\n\n\n\n\nSap_Flow\n\n\nDrought-sens-under\n\n\n0.86\n\n\n2.2e-09\n\n\n147\n\n\n\n\nSap_Flow\n\n\nDrought-tol-canopy\n\n\n0.91\n\n\n8.3e-07\n\n\n147\n\n\n\n\nSap_Flow\n\n\nDrought-tol-under\n\n\n0.90\n\n\n2.2e-07\n\n\n147\n\n\n\n\nTWaterFlux\n\n\nDrought-sens-canopy\n\n\n0.93\n\n\n1.1e-06\n\n\n147\n\n\n\n\nTWaterFlux\n\n\nDrought-sens-under\n\n\n0.93\n\n\n1.3e-06\n\n\n147\n\n\n\n\nTWaterFlux\n\n\nDrought-tol-canopy\n\n\n0.96\n\n\n1.3e-04\n\n\n147\n\n\n\n\nTWaterFlux\n\n\nDrought-tol-under\n\n\n0.98\n\n\n4.4e-02\n\n\n147\n\n\n\n\n\n\n\n\n\nQ-Q Plots\n\ndataset |&gt;\ngroup_by(Group) |&gt;\n  select(Sap_Flow, TWaterFlux, Group) |&gt;\n  plot_normality()"
  },
  {
    "objectID": "ExploringLikeDataAdventurer.html#produce-an-html-normality-summary",
    "href": "ExploringLikeDataAdventurer.html#produce-an-html-normality-summary",
    "title": "Exploring like a Data Adventurer",
    "section": "Produce an HTML Normality Summary",
    "text": "Produce an HTML Normality Summary\n\neda_web_report(dataset)\n\n\n\n\n\nMeredith, Laura, S. Nemiah Ladd, and Christiane Werner. 2021. “Data for \"Ecosystem Fluxes During Drought and Recovery in an Experimental Forest\".” University of Arizona Research Data Repository. https://doi.org/10.25422/AZU.DATA.14632593.V1."
  },
  {
    "objectID": "TransformingLikeDataTrans.html#purpose-of-this-chapter",
    "href": "TransformingLikeDataTrans.html#purpose-of-this-chapter",
    "title": "Transforming like a Data… Transformer",
    "section": "Purpose of this chapter",
    "text": "Purpose of this chapter\nUsing data transformation to correct non-normality in numerical data"
  },
  {
    "objectID": "TransformingLikeDataTrans.html#take-aways",
    "href": "TransformingLikeDataTrans.html#take-aways",
    "title": "Transforming like a Data… Transformer",
    "section": "Take-aways",
    "text": "Take-aways\n\nLoad and explore a data set with publication quality tables\nQuickly diagnose non-normality in data\nData transformation\nPrepare an HTML summary report showcasing data transformations"
  },
  {
    "objectID": "TransformingLikeDataTrans.html#required-setup",
    "href": "TransformingLikeDataTrans.html#required-setup",
    "title": "Transforming like a Data… Transformer",
    "section": "Required Setup",
    "text": "Required Setup\nWe first need to prepare our environment with the necessary packages\n\n# Sets the repository to download packages from\noptions(repos = list(CRAN = \"http://cran.rstudio.com/\"))\n\n# Sets the number of significant figures to two - e.g., 0.01\noptions(digits = 2)\n\n# Required package for quick package downloading and loading \ninstall.packages(\"pacman\")\n\n# Downloads and load required packages\npacman::p_load(dlookr, # Exploratory data analysis\n               forecast, # Needed for Box-Cox transformations\n               formattable, # HTML tables from R outputs\n               here, # Standardizes paths to data\n               kableExtra, # Alternative to formattable\n               knitr, # Needed to write HTML reports\n               missRanger, # To generate NAs\n               tidyverse) # Powerful data wrangling package suite"
  },
  {
    "objectID": "TransformingLikeDataTrans.html#load-and-examine-a-data-set",
    "href": "TransformingLikeDataTrans.html#load-and-examine-a-data-set",
    "title": "Transforming like a Data… Transformer",
    "section": "Load and Examine a Data Set",
    "text": "Load and Examine a Data Set\n\nLoad data and view\nExamine columns and data types\nExamine data normality\nDescribe properties of data\n\n\n# Let's load a data set from the diabetes data set\ndataset &lt;- read.csv(here(\"EDA_In_R_Book\", \"data\", \"diabetes.csv\")) |&gt;\n  # Add a categorical group\n  mutate(Age_group = ifelse(Age &gt;= 21 & Age &lt;= 30, \"Young\", \n                            ifelse(Age &gt; 30 & Age &lt;= 50, \"Middle\", \n                                   \"Elderly\")),\n         Age_group = fct_rev(Age_group))\n\n# What does the data look like?\ndataset |&gt;\n  head() |&gt;\n  formattable()\n\n\n\n\n\n\nPregnancies\n\n\nGlucose\n\n\nBloodPressure\n\n\nSkinThickness\n\n\nInsulin\n\n\nBMI\n\n\nDiabetesPedigreeFunction\n\n\nAge\n\n\nOutcome\n\n\nAge_group\n\n\n\n\n\n\n6\n\n\n148\n\n\n72\n\n\n35\n\n\n0\n\n\n34\n\n\n0.63\n\n\n50\n\n\n1\n\n\nMiddle\n\n\n\n\n1\n\n\n85\n\n\n66\n\n\n29\n\n\n0\n\n\n27\n\n\n0.35\n\n\n31\n\n\n0\n\n\nMiddle\n\n\n\n\n8\n\n\n183\n\n\n64\n\n\n0\n\n\n0\n\n\n23\n\n\n0.67\n\n\n32\n\n\n1\n\n\nMiddle\n\n\n\n\n1\n\n\n89\n\n\n66\n\n\n23\n\n\n94\n\n\n28\n\n\n0.17\n\n\n21\n\n\n0\n\n\nYoung\n\n\n\n\n0\n\n\n137\n\n\n40\n\n\n35\n\n\n168\n\n\n43\n\n\n2.29\n\n\n33\n\n\n1\n\n\nMiddle\n\n\n\n\n5\n\n\n116\n\n\n74\n\n\n0\n\n\n0\n\n\n26\n\n\n0.20\n\n\n30\n\n\n0\n\n\nYoung\n\n\n\n\n\n\n\n\n\nData Normality\nNormal distributions (bell curves) are a common data assumptions for many hypothesis testing statistics, in particular parametric statistics. Deviations from normality can either strongly skew the results or reduce the power to detect a significant statistical difference.\nHere are the distribution properties to know and consider:\n\nThe mean, median, and mode are the same value.\nDistribution symmetry at the mean.\nNormal distributions can be described by the mean and standard deviation.\n\nHere’s an example using the Glucose column in our dataset\n\nlibrary(ggpubr)\n\n# Function for data mode\ngetmode &lt;- function(v) {\n   uniqv &lt;- unique(v)\n   uniqv[which.max(tabulate(match(v, uniqv)))]\n}\n\ndataset |&gt; \n  ggplot(aes(x = BMI)) +\n  geom_histogram(fill = \"#4E84C4\", size = 2, bins = 40) +\n  geom_point(aes(y = 30, x = mean(BMI)), size = 5) +\n  geom_point(aes(y = 30, x = median(BMI)), size = 5) +\n  geom_point(aes(y = 30, x = getmode(BMI)), size = 5) +\n  geom_vline(aes(xintercept = mean(BMI) - sd(BMI)), size = 1.5, linetype = \"dashed\") + \n  geom_vline(aes(xintercept = mean(BMI) + sd(BMI)), size = 1.5, linetype = \"dashed\") +\n  geom_segment(aes(y = 30, yend = 30, x = mean(BMI) - sd(BMI), xend = mean(BMI) + sd(BMI)), size = 1.5) +\n  geom_segment(aes(y = 32, yend = 45, x = mean(BMI) + 1.5, xend = 50)) +\n  geom_segment(aes(y = 30, yend = 30, x = mean(BMI) + sd(BMI) + 1, xend = 50)) +\n  annotate(geom = \"text\", x = 50.5, y = 45, label = \"Mean, \\nMedian, \\nMode = 32\", size = 5, hjust = 0) +\n  annotate(geom = \"text\", x = 50.5, y = 30, label = \"SD = 7.9\", size = 5, hjust = 0) +\n  ylab(\"Count\") +\n  theme(axis.title.y = element_blank()) +\n  theme_pubclean(base_size = 16)\n\n\n\n\n\n\n\nDescribing Properties of our Data (Refined)\n\nSkewness\nThe symmetry of the distribution\nSee Introduction 4.3 for more information about these values\n\ndataset |&gt;\n  select(Glucose, Insulin, BMI, SkinThickness) |&gt;\n  describe() |&gt;\n  select(described_variables, skewness) |&gt;\n  formattable()\n\n\n\n\n\n\ndescribed_variables\n\n\nskewness\n\n\n\n\n\n\nGlucose\n\n\n0.17\n\n\n\n\nInsulin\n\n\n2.27\n\n\n\n\nBMI\n\n\n-0.43\n\n\n\n\nSkinThickness\n\n\n0.11\n\n\n\n\n\n\n\nNote that we will remove the other percentiles to produce a cleaner output\n\ndescribes_variables: name of the column being described\nskewness: skewness"
  },
  {
    "objectID": "TransformingLikeDataTrans.html#testing-normality-accelerated",
    "href": "TransformingLikeDataTrans.html#testing-normality-accelerated",
    "title": "Transforming like a Data… Transformer",
    "section": "Testing Normality (Accelerated)",
    "text": "Testing Normality (Accelerated)\n\nQ-Q plots\nTesting overall normality of two columns\nTesting normality of groups\n\nNote that you can also use normality() to run Shapiro-Wilk tests, but since this test is not viable at N &lt; 20, I recommend Q-Q plots.\n\n\nQ-Q Plots\nPlots of the quartiles of a target data set against the predicted quartiles from a normal distribution.\nNotably, plot_normality() will show you the logaritmic and skewed transformations (more below)\n\ndataset |&gt;\nplot_normality(Glucose, Insulin, Age)"
  },
  {
    "objectID": "TransformingLikeDataTrans.html#normality-within-groups",
    "href": "TransformingLikeDataTrans.html#normality-within-groups",
    "title": "Transforming like a Data… Transformer",
    "section": "Normality within Groups",
    "text": "Normality within Groups\nLooking within Age_group at the subgroup normality\n\nQ-Q Plots\n\ndataset %&gt;%\n  group_by(Age_group) %&gt;%\n  select(Glucose, Insulin) %&gt;%\n  plot_normality()"
  },
  {
    "objectID": "TransformingLikeDataTrans.html#transforming-data",
    "href": "TransformingLikeDataTrans.html#transforming-data",
    "title": "Transforming like a Data… Transformer",
    "section": "Transforming Data",
    "text": "Transforming Data\nYour data could be more easily interpreted with a transformation, since not all relationships in nature follow a linear relationship - i.e., many biological phenomena follow a power law (or logarithmic curve), where they do not scale linearly.\nWe will try to transform the Insulin column with through several approaches and discuss the pros and cons of each. First however, we will remove 0 values, because Insulin values are impossible…\n\nInsMod &lt;- dataset |&gt;\n  filter(Insulin &gt; 0)\n\n\n\nSquare-root, Cube-root, and Logarithmic Transformations\nResolving Skewness using transform().\n“sqrt”: square-root transformation. \\(\\sqrt x\\) (moderate skew)\n“log”: log transformation. \\(log(x)\\) (greater skew)\n“log+1”: log transformation. \\(log(x + 1)\\). Used for values that contain 0.\n“1/x”: inverse transformation. \\(1/x\\) (severe skew)\n“x^2”: squared transformation. \\(x^2\\)\n“x^3”: cubed transformation. \\(x^3\\)\nWe will compare the sqrt, log+1, 1/x (inverse), x^2, and x^3 transformations. Note that you would have to add a constant to use the log transformation, so it is easier to use the log+1 instead. You however need to add a constant to both the sqrt and 1/x transformations because they don’t include zeros and will otherwise skew the results. Here we removed zeros a priori.\n\n\nSquare-root Transformation\n\nsqrtIns &lt;- transform(InsMod$Insulin, method = \"sqrt\") \n\nsummary(sqrtIns)\n\n* Resolving Skewness with sqrt\n\n* Information of Transformation (before vs after)\n         Original Transformation\nn           394.0         394.00\nna            0.0           0.00\nmean        155.5          11.75\nsd          118.8           4.17\nse_mean       6.0           0.21\nIQR         113.8           5.05\nskewness      2.2           1.01\nkurtosis      6.4           1.46\np00          14.0           3.74\np01          18.0           4.24\np05          41.7           6.45\np10          50.3           7.09\np20          69.2           8.32\np25          76.2           8.73\np30          87.9           9.38\np40         105.0          10.25\np50         125.0          11.18\np60         145.8          12.07\np70         176.0          13.27\np75         190.0          13.78\np80         210.0          14.49\np90         292.4          17.10\np95         395.5          19.89\np99         580.5          24.09\np100        846.0          29.09\n\n\n\nsqrtIns |&gt;\n  plot()\n\n\n\n\n\n\n\nLogarithmic (+1) Transformation\n\nLog1Ins &lt;- transform(InsMod$Insulin, method = \"log+1\") \n\nsummary(Log1Ins)\n\n* Resolving Skewness with log+1\n\n* Information of Transformation (before vs after)\n         Original Transformation\nn           394.0        394.000\nna            0.0          0.000\nmean        155.5          4.818\nsd          118.8          0.691\nse_mean       6.0          0.035\nIQR         113.8          0.905\nskewness      2.2         -0.088\nkurtosis      6.4          0.237\np00          14.0          2.708\np01          18.0          2.944\np05          41.7          3.753\np10          50.3          3.938\np20          69.2          4.251\np25          76.2          4.347\np30          87.9          4.488\np40         105.0          4.663\np50         125.0          4.836\np60         145.8          4.989\np70         176.0          5.176\np75         190.0          5.252\np80         210.0          5.352\np90         292.4          5.682\np95         395.5          5.983\np99         580.5          6.366\np100        846.0          6.742\n\n\n\nLog1Ins |&gt;\n  plot()\n\n\n\n\n\n\n\nInverse Transformation\n\nInvIns &lt;- transform(InsMod$Insulin, method = \"1/x\") \n\nsummary(InvIns)\n\n* Resolving Skewness with 1/x\n\n* Information of Transformation (before vs after)\n         Original Transformation\nn           394.0        3.9e+02\nna            0.0        0.0e+00\nmean        155.5        1.1e-02\nsd          118.8        9.0e-03\nse_mean       6.0        4.6e-04\nIQR         113.8        7.9e-03\nskewness      2.2        3.2e+00\nkurtosis      6.4        1.5e+01\np00          14.0        1.2e-03\np01          18.0        1.7e-03\np05          41.7        2.5e-03\np10          50.3        3.4e-03\np20          69.2        4.8e-03\np25          76.2        5.3e-03\np30          87.9        5.7e-03\np40         105.0        6.9e-03\np50         125.0        8.0e-03\np60         145.8        9.5e-03\np70         176.0        1.1e-02\np75         190.0        1.3e-02\np80         210.0        1.4e-02\np90         292.4        2.0e-02\np95         395.5        2.4e-02\np99         580.5        5.6e-02\np100        846.0        7.1e-02\n\n\n\nInvIns |&gt;\n  plot()\n\n\n\n\n\n\n\nSquared Transformation\n\nSqrdIns &lt;- transform(InsMod$Insulin, method = \"x^2\") \n\nsummary(SqrdIns)\n\n* Resolving Skewness with x^2\n\n* Information of Transformation (before vs after)\n         Original Transformation\nn           394.0        3.9e+02\nna            0.0        0.0e+00\nmean        155.5        3.8e+04\nsd          118.8        7.2e+04\nse_mean       6.0        3.7e+03\nIQR         113.8        3.0e+04\nskewness      2.2        4.8e+00\nkurtosis      6.4        3.1e+01\np00          14.0        2.0e+02\np01          18.0        3.2e+02\np05          41.7        1.7e+03\np10          50.3        2.5e+03\np20          69.2        4.8e+03\np25          76.2        5.8e+03\np30          87.9        7.7e+03\np40         105.0        1.1e+04\np50         125.0        1.6e+04\np60         145.8        2.1e+04\np70         176.0        3.1e+04\np75         190.0        3.6e+04\np80         210.0        4.4e+04\np90         292.4        8.5e+04\np95         395.5        1.6e+05\np99         580.5        3.4e+05\np100        846.0        7.2e+05\n\n\n\nSqrdIns |&gt;\n  plot()\n\n\n\n\n\n\n\nCubed Transformation\n\nCubeIns &lt;- transform(InsMod$Insulin, method = \"x^3\") \n\nsummary(CubeIns)\n\n* Resolving Skewness with x^3\n\n* Information of Transformation (before vs after)\n         Original Transformation\nn           394.0        3.9e+02\nna            0.0        0.0e+00\nmean        155.5        1.4e+07\nsd          118.8        4.8e+07\nse_mean       6.0        2.4e+06\nIQR         113.8        6.4e+06\nskewness      2.2        7.7e+00\nkurtosis      6.4        7.7e+01\np00          14.0        2.7e+03\np01          18.0        5.8e+03\np05          41.7        7.2e+04\np10          50.3        1.3e+05\np20          69.2        3.3e+05\np25          76.2        4.4e+05\np30          87.9        6.8e+05\np40         105.0        1.2e+06\np50         125.0        2.0e+06\np60         145.8        3.1e+06\np70         176.0        5.5e+06\np75         190.0        6.9e+06\np80         210.0        9.3e+06\np90         292.4        2.5e+07\np95         395.5        6.2e+07\np99         580.5        2.0e+08\np100        846.0        6.1e+08\n\n\n\nCubeIns |&gt;\n  plot()\n\n\n\n\n\n\n\n\nBox-cox Transformation\nThere are several transformations, each with it’s own “criteria”, and they don’t always fix extremely skewed data. Instead, you can just choose the Box-Cox transformation which searches for the the best lambda value that maximizes the log-likelihood (basically, what power transformation is best). The benefit is that you should have normally distributed data after, but the power relationship might be pretty abstract (i.e., what would a transformation of x^0.12 be interpreted as in your system?..)\n\nBoxCoxIns &lt;- transform(InsMod$Insulin, method = \"Box-Cox\") \n\nsummary(BoxCoxIns)\n\n* Resolving Skewness with Box-Cox\n\n* Information of Transformation (before vs after)\n         Original Transformation\nn           394.0        394.000\nna            0.0          0.000\nmean        155.5          3.011\nsd          118.8          0.262\nse_mean       6.0          0.013\nIQR         113.8          0.335\nskewness      2.2         -0.630\nkurtosis      6.4          1.003\np00          14.0          2.027\np01          18.0          2.168\np05          41.7          2.588\np10          50.3          2.673\np20          69.2          2.808\np25          76.2          2.848\np30          87.9          2.904\np40         105.0          2.973\np50         125.0          3.037\np60         145.8          3.092\np70         176.0          3.157\np75         190.0          3.183\np80         210.0          3.216\np90         292.4          3.320\np95         395.5          3.409\np99         580.5          3.515\np100        846.0          3.610\n\n\n\nBoxCoxIns |&gt;\n  plot()"
  },
  {
    "objectID": "TransformingLikeDataTrans.html#produce-an-html-transformation-summary",
    "href": "TransformingLikeDataTrans.html#produce-an-html-transformation-summary",
    "title": "Transforming like a Data… Transformer",
    "section": "Produce an HTML Transformation Summary",
    "text": "Produce an HTML Transformation Summary\n\ntransformation_web_report(dataset)"
  },
  {
    "objectID": "ImputingLikeDataScientist.html#purpose-of-this-chapter",
    "href": "ImputingLikeDataScientist.html#purpose-of-this-chapter",
    "title": "Imputing like a Data Scientist",
    "section": "Purpose of this chapter",
    "text": "Purpose of this chapter\nExploring, visualizing, and imputing outliers and missing values (NAs) in a novel data set and produce publication quality graphs and tables\nIMPORTANT NOTE: imputation should only be used when missing data is unavoidable and probably limited to 10% of your data being outliers / missing data (though some argue imputation is necessary between 30-60%). Ask what the cause is for the outlier and missing data."
  },
  {
    "objectID": "ImputingLikeDataScientist.html#take-aways",
    "href": "ImputingLikeDataScientist.html#take-aways",
    "title": "Imputing like a Data Scientist",
    "section": "Take-aways",
    "text": "Take-aways\n\nLoad and explore a data set with publication quality tables\nThoroughly diagnose outliers and missing values\nImpute outliers and missing values"
  },
  {
    "objectID": "ImputingLikeDataScientist.html#required-setup",
    "href": "ImputingLikeDataScientist.html#required-setup",
    "title": "Imputing like a Data Scientist",
    "section": "Required Setup",
    "text": "Required Setup\nWe first need to prepare our environment with the necessary packages and set a global theme for publishable plots in ggplot()\n\n# Sets the repository to download packages from\noptions(repos = list(CRAN = \"http://cran.rstudio.com/\"))\n\n# Sets the number of significant figures to two - e.g., 0.01\noptions(digits = 2)\n\n# Required package for quick package downloading and loading \ninstall.packages(\"pacman\")\n\npacman::p_load(colorblindr, # Colorblind friendly pallettes\n               cluster, # K cluster analyses\n               dlookr, # Exploratory data analysis\n               formattable, # HTML tables from R outputs\n               ggfortify, # Plotting tools for stats\n               ggpubr, # Publishable ggplots\n               here, # Standardizes paths to data\n               kableExtra, # Alternative to formattable\n               knitr, # Needed to write HTML reports\n               missRanger, # To generate NAs\n               plotly, # Visualization package\n               rattle, # Decision tree visualization\n               rpart, # rpart algorithm\n               tidyverse, # Powerful data wrangling package suite\n               visdat) # Another EDA visualization package\n\n# Set global ggplot() theme\n# Theme pub_clean() from the ggpubr package with base text size = 16\ntheme_set(theme_pubclean(base_size = 16)) \n# All axes titles to their respective far right sides\ntheme_update(axis.title = element_text(hjust = 1))\n# Remove axes ticks\ntheme_update(axis.ticks = element_blank()) \n# Remove legend key\ntheme_update(legend.key = element_blank())"
  },
  {
    "objectID": "ImputingLikeDataScientist.html#load-and-examine-a-data-set",
    "href": "ImputingLikeDataScientist.html#load-and-examine-a-data-set",
    "title": "Imputing like a Data Scientist",
    "section": "Load and Examine a Data Set",
    "text": "Load and Examine a Data Set\n\n# Let's the diabetes data set\ndataset &lt;- read.csv(here(\"EDA_In_R_Book\", \"data\", \"diabetes.csv\")) |&gt;\n  # Add a categorical group\n  mutate(Age_group = ifelse(Age &gt;= 21 & Age &lt;= 30, \"Young\", \n                            ifelse(Age &gt; 30 & Age &lt;=50, \"Middle\", \n                                   \"Elderly\")),\n         Age_group = fct_rev(Age_group))\n\n# What does the data look like?\ndataset |&gt;\n  head() |&gt;\n  formattable()\n\n\n\n\n\n\nPregnancies\n\n\nGlucose\n\n\nBloodPressure\n\n\nSkinThickness\n\n\nInsulin\n\n\nBMI\n\n\nDiabetesPedigreeFunction\n\n\nAge\n\n\nOutcome\n\n\nAge_group\n\n\n\n\n\n\n6\n\n\n148\n\n\n72\n\n\n35\n\n\n0\n\n\n34\n\n\n0.63\n\n\n50\n\n\n1\n\n\nMiddle\n\n\n\n\n1\n\n\n85\n\n\n66\n\n\n29\n\n\n0\n\n\n27\n\n\n0.35\n\n\n31\n\n\n0\n\n\nMiddle\n\n\n\n\n8\n\n\n183\n\n\n64\n\n\n0\n\n\n0\n\n\n23\n\n\n0.67\n\n\n32\n\n\n1\n\n\nMiddle\n\n\n\n\n1\n\n\n89\n\n\n66\n\n\n23\n\n\n94\n\n\n28\n\n\n0.17\n\n\n21\n\n\n0\n\n\nYoung\n\n\n\n\n0\n\n\n137\n\n\n40\n\n\n35\n\n\n168\n\n\n43\n\n\n2.29\n\n\n33\n\n\n1\n\n\nMiddle\n\n\n\n\n5\n\n\n116\n\n\n74\n\n\n0\n\n\n0\n\n\n26\n\n\n0.20\n\n\n30\n\n\n0\n\n\nYoung"
  },
  {
    "objectID": "ImputingLikeDataScientist.html#diagnose-your-data",
    "href": "ImputingLikeDataScientist.html#diagnose-your-data",
    "title": "Imputing like a Data Scientist",
    "section": "Diagnose your Data",
    "text": "Diagnose your Data\n\n# What are the properties of the data\ndataset |&gt;\n  diagnose() |&gt;\n  formattable()\n\n\n\n\n\n\nvariables\n\n\ntypes\n\n\nmissing_count\n\n\nmissing_percent\n\n\nunique_count\n\n\nunique_rate\n\n\n\n\n\n\nPregnancies\n\n\ninteger\n\n\n0\n\n\n0\n\n\n17\n\n\n0.0221\n\n\n\n\nGlucose\n\n\ninteger\n\n\n0\n\n\n0\n\n\n136\n\n\n0.1771\n\n\n\n\nBloodPressure\n\n\ninteger\n\n\n0\n\n\n0\n\n\n47\n\n\n0.0612\n\n\n\n\nSkinThickness\n\n\ninteger\n\n\n0\n\n\n0\n\n\n51\n\n\n0.0664\n\n\n\n\nInsulin\n\n\ninteger\n\n\n0\n\n\n0\n\n\n186\n\n\n0.2422\n\n\n\n\nBMI\n\n\nnumeric\n\n\n0\n\n\n0\n\n\n248\n\n\n0.3229\n\n\n\n\nDiabetesPedigreeFunction\n\n\nnumeric\n\n\n0\n\n\n0\n\n\n517\n\n\n0.6732\n\n\n\n\nAge\n\n\ninteger\n\n\n0\n\n\n0\n\n\n52\n\n\n0.0677\n\n\n\n\nOutcome\n\n\ninteger\n\n\n0\n\n\n0\n\n\n2\n\n\n0.0026\n\n\n\n\nAge_group\n\n\nfactor\n\n\n0\n\n\n0\n\n\n3\n\n\n0.0039\n\n\n\n\n\n\n\n\nvariables: name of each variable\ntypes: data type of each variable\nmissing_count: number of missing values\nmissing_percent: percentage of missing values\nunique_count: number of unique values\nunique_rate: rate of unique value - unique_count / number of observations"
  },
  {
    "objectID": "ImputingLikeDataScientist.html#diagnose-outliers",
    "href": "ImputingLikeDataScientist.html#diagnose-outliers",
    "title": "Imputing like a Data Scientist",
    "section": "Diagnose Outliers",
    "text": "Diagnose Outliers\nThere are several numerical variables that have outliers above, let’s see what the data look like with and without them\n\nCreate a table with columns containing outliers\nPlot outliers in a box plot and histogram\n\n\n# Table showing outliers\ndataset |&gt;\n  diagnose_outlier() |&gt;\n  filter(outliers_ratio &gt; 0) |&gt;  \n  mutate(rate = outliers_mean / with_mean) |&gt;\n  arrange(desc(rate)) |&gt; \n  select(-outliers_cnt) |&gt;\n  formattable()\n\n\n\n\n\n\nvariables\n\n\noutliers_ratio\n\n\noutliers_mean\n\n\nwith_mean\n\n\nwithout_mean\n\n\nrate\n\n\n\n\n\n\nInsulin\n\n\n4.43\n\n\n457.0\n\n\n79.80\n\n\n62.33\n\n\n5.73\n\n\n\n\nSkinThickness\n\n\n0.13\n\n\n99.0\n\n\n20.54\n\n\n20.43\n\n\n4.82\n\n\n\n\nPregnancies\n\n\n0.52\n\n\n15.0\n\n\n3.85\n\n\n3.79\n\n\n3.90\n\n\n\n\nDiabetesPedigreeFunction\n\n\n3.78\n\n\n1.5\n\n\n0.47\n\n\n0.43\n\n\n3.27\n\n\n\n\nAge\n\n\n1.17\n\n\n70.0\n\n\n33.24\n\n\n32.81\n\n\n2.11\n\n\n\n\nBMI\n\n\n2.47\n\n\n23.7\n\n\n31.99\n\n\n32.20\n\n\n0.74\n\n\n\n\nBloodPressure\n\n\n5.86\n\n\n19.2\n\n\n69.11\n\n\n72.21\n\n\n0.28\n\n\n\n\nGlucose\n\n\n0.65\n\n\n0.0\n\n\n120.89\n\n\n121.69\n\n\n0.00\n\n\n\n\n\n\n\n\n# Boxplots and histograms of data with and without outliers\ndataset |&gt;\n  select(find_outliers(dataset)) |&gt;\n           plot_outlier()"
  },
  {
    "objectID": "ImputingLikeDataScientist.html#basic-exploration-of-missing-values-nas",
    "href": "ImputingLikeDataScientist.html#basic-exploration-of-missing-values-nas",
    "title": "Imputing like a Data Scientist",
    "section": "Basic Exploration of Missing Values (NAs)",
    "text": "Basic Exploration of Missing Values (NAs)\n\nTable showing the extent of NAs in columns containing them\n\n\n# Randomly generate NAs for 30\nna.dataset &lt;- dataset |&gt;\n  generateNA(p = 0.3)\n\n# First six rows\nna.dataset |&gt;\nhead() |&gt;\n  formattable()\n\n\n\n\n\n\nPregnancies\n\n\nGlucose\n\n\nBloodPressure\n\n\nSkinThickness\n\n\nInsulin\n\n\nBMI\n\n\nDiabetesPedigreeFunction\n\n\nAge\n\n\nOutcome\n\n\nAge_group\n\n\n\n\n\n\n6\n\n\nNA\n\n\n72\n\n\n35\n\n\nNA\n\n\n34\n\n\n0.63\n\n\n50\n\n\n1\n\n\nNA\n\n\n\n\nNA\n\n\n85\n\n\n66\n\n\n29\n\n\nNA\n\n\n27\n\n\n0.35\n\n\n31\n\n\n0\n\n\nMiddle\n\n\n\n\n8\n\n\n183\n\n\n64\n\n\n0\n\n\nNA\n\n\n23\n\n\nNA\n\n\n32\n\n\n1\n\n\nMiddle\n\n\n\n\n1\n\n\n89\n\n\n66\n\n\n23\n\n\n94\n\n\n28\n\n\n0.17\n\n\n21\n\n\n0\n\n\nNA\n\n\n\n\n0\n\n\n137\n\n\n40\n\n\n35\n\n\n168\n\n\nNA\n\n\nNA\n\n\n33\n\n\n1\n\n\nMiddle\n\n\n\n\n5\n\n\n116\n\n\n74\n\n\n0\n\n\n0\n\n\n26\n\n\n0.20\n\n\nNA\n\n\n0\n\n\nYoung\n\n\n\n\n\n\n# Create the NA table\nna.dataset |&gt;\n  plot_na_pareto(only_na = TRUE, plot = FALSE) |&gt;\n  formattable() # Publishable table\n\n\n\n\n\n\nvariable\n\n\nfrequencies\n\n\nratio\n\n\ngrade\n\n\ncumulative\n\n\n\n\n\n\nAge\n\n\n230\n\n\n0.3\n\n\nBad\n\n\n10\n\n\n\n\nAge_group\n\n\n230\n\n\n0.3\n\n\nBad\n\n\n20\n\n\n\n\nBMI\n\n\n230\n\n\n0.3\n\n\nBad\n\n\n30\n\n\n\n\nBloodPressure\n\n\n230\n\n\n0.3\n\n\nBad\n\n\n40\n\n\n\n\nDiabetesPedigreeFunction\n\n\n230\n\n\n0.3\n\n\nBad\n\n\n50\n\n\n\n\nGlucose\n\n\n230\n\n\n0.3\n\n\nBad\n\n\n60\n\n\n\n\nInsulin\n\n\n230\n\n\n0.3\n\n\nBad\n\n\n70\n\n\n\n\nOutcome\n\n\n230\n\n\n0.3\n\n\nBad\n\n\n80\n\n\n\n\nPregnancies\n\n\n230\n\n\n0.3\n\n\nBad\n\n\n90\n\n\n\n\nSkinThickness\n\n\n230\n\n\n0.3\n\n\nBad\n\n\n100\n\n\n\n\n\n\n\n\nPlots showing the frequency of missing values\n\n\n# Plot the insersect of the columns with missing values\n# This plot visualizes the table above\nna.dataset |&gt;\n  plot_na_pareto(only_na = TRUE)"
  },
  {
    "objectID": "ImputingLikeDataScientist.html#advanced-exploration-of-missing-values-nas",
    "href": "ImputingLikeDataScientist.html#advanced-exploration-of-missing-values-nas",
    "title": "Imputing like a Data Scientist",
    "section": "Advanced Exploration of Missing Values (NAs)",
    "text": "Advanced Exploration of Missing Values (NAs)\n\nIntersect plot that shows, for every combination of columns relevant, how many missing values are common\nOrange boxes are the columns in question\nx axis (top green bar plots) show the number of missing values in that column\ny axis (right green bars) show the number of missing values in the columns in orange blocks\n\n\n# Plot the intersect of the 5 columns with the most missing values\n# This means that some combinations of columns have missing values in the same row\nna.dataset |&gt;\n  select(BloodPressure, Glucose, Age) |&gt;\n  plot_na_intersect(only_na = TRUE) \n\n\n\n\n\n\nDetermining if NA Observations are the Same\n\nMissing values can be the same observation across several columns, this is not shown above\nThe visdat package can solve this with the vis_miss() function which shows the rows with missing values through ggplotly()\nHere we will show ALL columns with NAs, and you can zoom into individual rows (interactive plot)\nNOTE: This line will make the HTML rendering take a while…\n\n\n# Interactive plotly() plot of all NA values to examine every row\nna.dataset |&gt;\n select(BloodPressure, Glucose, Age) |&gt;\n vis_miss() |&gt;\n ggplotly()"
  },
  {
    "objectID": "ImputingLikeDataScientist.html#impute-outliers-and-nas",
    "href": "ImputingLikeDataScientist.html#impute-outliers-and-nas",
    "title": "Imputing like a Data Scientist",
    "section": "Impute Outliers and NAs",
    "text": "Impute Outliers and NAs\nRemoving outliers and NAs can be tricky, but there are methods to do so. I will go over several, and discuss benefits and costs to each.\nThe principle goal for all imputation is to find the method that does not change the distribution too much (or oddly).\n\n\nClassifying Outliers\nBefore imputing outliers, you will want to diagnose whether it’s they are natural outliers or not. We will be looking at “Insulin” for example across Age_group, because there are outliers and several NAs, which we will impute below.\n\n# Box plot\ndataset %&gt;% # Set the simulated normal data as a data frame\n  ggplot(aes(x = Insulin, y = Age_group, fill = Age_group)) + # Create a ggplot\n  geom_boxplot(width = 0.5, outlier.size = 2, outlier.alpha = 0.5) +\n  xlab(\"Insulin (mg/dL)\") +  # Relabel the x axis label\n  ylab(\"Age group\") + # Remove the y axis label\n  scale_fill_OkabeIto() + # Change the color scheme for the fill criteria\n  theme(legend.position = \"none\")  # Remove the legend \n\n\n\n\nNow let’s say that we want to impute extreme values and remove outliers that don’t make sense, such as Insulin levels &gt; 600 mg/dL: values greater than this induce a diabetic coma.\nWe remove outliers using imputate_outlier() and replace them with values that are estimates based on the existing data\n\nmean: arithmetic mean\nmedian: median\nmode: mode\ncapping: Impute the upper outliers with 95 percentile, and impute the bottom outliers with 5 percentile - aka Winsorizing\n\n\n\n\nMean Imputation\nThe mean of the observed values for each variable is computed and the outliers for that variable are imputed by this mean\n\n# Raw summary, output suppressed\nmean_out_imp_insulin &lt;- dataset |&gt;\n  select(Insulin) |&gt;\n  filter(Insulin &lt; 600) |&gt;\n  imputate_outlier(Insulin, method = \"mean\")\n\n# Output showing the summary statistics of our imputation\nmean_out_imp_insulin |&gt;\n  summary() \n\nImpute outliers with mean\n\n* Information of Imputation (before vs after)\n                    Original Imputation\ndescribed_variables \"value\"  \"value\"   \nn                   \"764\"    \"764\"     \nna                  \"0\"      \"0\"       \nmean                \"76\"     \"63\"      \nsd                  \"106\"    \" 77\"     \nse_mean             \"3.8\"    \"2.8\"     \nIQR                 \"126\"    \"110\"     \nskewness            \"1.8\"    \"1.1\"     \nkurtosis            \"3.90\"   \"0.28\"    \np00                 \"0\"      \"0\"       \np01                 \"0\"      \"0\"       \np05                 \"0\"      \"0\"       \np10                 \"0\"      \"0\"       \np20                 \"0\"      \"0\"       \np25                 \"0\"      \"0\"       \np30                 \"0\"      \"0\"       \np40                 \"0\"      \"0\"       \np50                 \"24\"     \"24\"      \np60                 \"71\"     \"71\"      \np70                 \"105\"    \" 92\"     \np75                 \"126\"    \"110\"     \np80                 \"147\"    \"130\"     \np90                 \"207\"    \"180\"     \np95                 \"285\"    \"215\"     \np99                 \"482\"    \"284\"     \np100                \"579\"    \"310\"     \n\n\n\n# Visualization of the mean imputation\nmean_out_imp_insulin |&gt;\n  plot()\n\n\n\n\n\n\n\nMedian Imputation\nThe median of the observed values for each variable is computed and the outliers for that variable are imputed by this median\n\n# Raw summary, output suppressed\nmed_out_imp_insulin &lt;- dataset |&gt;\n  select(Insulin) |&gt;\n  filter(Insulin &lt; 600) |&gt;\n  imputate_outlier(Insulin, method = \"median\")\n\n# Output showing the summary statistics of our imputation\nmed_out_imp_insulin |&gt;\n  summary()\n\nImpute outliers with median\n\n* Information of Imputation (before vs after)\n                    Original Imputation\ndescribed_variables \"value\"  \"value\"   \nn                   \"764\"    \"764\"     \nna                  \"0\"      \"0\"       \nmean                \"76\"     \"60\"      \nsd                  \"106\"    \" 77\"     \nse_mean             \"3.8\"    \"2.8\"     \nIQR                 \"126\"    \"110\"     \nskewness            \"1.8\"    \"1.1\"     \nkurtosis            \"3.90\"   \"0.35\"    \np00                 \"0\"      \"0\"       \np01                 \"0\"      \"0\"       \np05                 \"0\"      \"0\"       \np10                 \"0\"      \"0\"       \np20                 \"0\"      \"0\"       \np25                 \"0\"      \"0\"       \np30                 \"0\"      \"0\"       \np40                 \"0\"      \"0\"       \np50                 \"24\"     \"24\"      \np60                 \"71\"     \"56\"      \np70                 \"105\"    \" 92\"     \np75                 \"126\"    \"110\"     \np80                 \"147\"    \"130\"     \np90                 \"207\"    \"180\"     \np95                 \"285\"    \"215\"     \np99                 \"482\"    \"284\"     \np100                \"579\"    \"310\"     \n\n\n\n# Visualization of the median imputation\nmed_out_imp_insulin |&gt;\n  plot()\n\n\n\n\n\n\nPros & Cons of Using the Mean or Median Imputation\nPros:\n\nEasy and fast.\nWorks well with small numerical datasets.\n\nCons:\n\nDoesn’t factor the correlations between variables. It only works on the column level.\nWill give poor results on encoded categorical variables (do NOT use it on categorical variables).\nNot very accurate.\nDoesn’t account for the uncertainty in the imputations.\n\n\n\n\n\nMode Imputation\nThe mode of the observed values for each variable is computed and the outliers for that variable are imputed by this mode\n\n# Raw summary, output suppressed\nmode_out_imp_insulin &lt;- dataset |&gt;\n  select(Insulin) |&gt;\n  filter(Insulin &lt; 600) |&gt;\n  imputate_outlier(Insulin, method = \"mode\")\n\n# Output showing the summary statistics of our imputation\nmode_out_imp_insulin |&gt;\n  summary()\n\nImpute outliers with mode\n\n* Information of Imputation (before vs after)\n                    Original Imputation\ndescribed_variables \"value\"  \"value\"   \nn                   \"764\"    \"764\"     \nna                  \"0\"      \"0\"       \nmean                \"76\"     \"59\"      \nsd                  \"106\"    \" 78\"     \nse_mean             \"3.8\"    \"2.8\"     \nIQR                 \"126\"    \"110\"     \nskewness            \"1.8\"    \"1.1\"     \nkurtosis            \"3.90\"   \"0.32\"    \np00                 \"0\"      \"0\"       \np01                 \"0\"      \"0\"       \np05                 \"0\"      \"0\"       \np10                 \"0\"      \"0\"       \np20                 \"0\"      \"0\"       \np25                 \"0\"      \"0\"       \np30                 \"0\"      \"0\"       \np40                 \"0\"      \"0\"       \np50                 \"24\"     \" 0\"      \np60                 \"71\"     \"56\"      \np70                 \"105\"    \" 92\"     \np75                 \"126\"    \"110\"     \np80                 \"147\"    \"130\"     \np90                 \"207\"    \"180\"     \np95                 \"285\"    \"215\"     \np99                 \"482\"    \"284\"     \np100                \"579\"    \"310\"     \n\n\n\n# Visualization of the mode imputation\nmode_out_imp_insulin |&gt;\nplot()\n\n\n\n\n\n\nPros & Cons of Using the Mode Imputation\nPros:\n\nWorks well with categorical variables.\n\nCons:\n\nIt also doesn’t factor the correlations between variables.\nIt can introduce bias in the data.\n\n\n\n\n\nCapping Imputation (aka Winsorizing)\nThe Percentile Capping is a method of Imputing the outlier values by replacing those observations outside the lower limit with the value of 5th percentile and those that lie above the upper limit, with the value of 95th percentile of the same dataset.\n\n# Raw summary, output suppressed\ncap_out_imp_insulin &lt;- dataset |&gt;\n  select(Insulin) |&gt;\n  filter(Insulin &lt; 600) |&gt;\n  imputate_outlier(Insulin, method = \"capping\")\n\n# Output showing the summary statistics of our imputation\ncap_out_imp_insulin |&gt;\n  summary()\n\nImpute outliers with capping\n\n* Information of Imputation (before vs after)\n                    Original Imputation\ndescribed_variables \"value\"  \"value\"   \nn                   \"764\"    \"764\"     \nna                  \"0\"      \"0\"       \nmean                \"76\"     \"71\"      \nsd                  \"106\"    \" 89\"     \nse_mean             \"3.8\"    \"3.2\"     \nIQR                 \"126\"    \"126\"     \nskewness            \"1.8\"    \"1.1\"     \nkurtosis            \"3.895\"  \"0.017\"   \np00                 \"0\"      \"0\"       \np01                 \"0\"      \"0\"       \np05                 \"0\"      \"0\"       \np10                 \"0\"      \"0\"       \np20                 \"0\"      \"0\"       \np25                 \"0\"      \"0\"       \np30                 \"0\"      \"0\"       \np40                 \"0\"      \"0\"       \np50                 \"24\"     \"24\"      \np60                 \"71\"     \"71\"      \np70                 \"105\"    \"105\"     \np75                 \"126\"    \"126\"     \np80                 \"147\"    \"147\"     \np90                 \"207\"    \"207\"     \np95                 \"285\"    \"285\"     \np99                 \"482\"    \"285\"     \np100                \"579\"    \"310\"     \n\n\n\n# Visualization of the capping imputation\ncap_out_imp_insulin |&gt;\n  plot()\n\n\n\n\n\n\nPros and Cons of Capping\nPros:\n\nNot influenced by extreme values\n\nCons:\n\nCapping only modifies the smallest and largest values slightly. This is generally not a good idea since it means we’re just modifying data values for the sake of modifications.\nIf no extreme outliers are present, Winsorization may be unnecessary."
  },
  {
    "objectID": "ImputingLikeDataScientist.html#imputing-nas",
    "href": "ImputingLikeDataScientist.html#imputing-nas",
    "title": "Imputing like a Data Scientist",
    "section": "Imputing NAs",
    "text": "Imputing NAs\nI will only be addressing a subset of methods for NA imputation using imputate_na() (but note you can use mean, median, and mode as well):\n\nknn: K-nearest neighbors (KNN)\nrpart: Recursive Partitioning and Regression Trees (rpart)\nmice: Multivariate Imputation by Chained Equations (MICE)\n\nSince our normal dataset has no NA values, we will use the na.dataset we created earlier.\n\n\nK-Nearest Neighbor (KNN) Imputation\nKNN is a machine learning algorithm that classifies data by similarity. This in effect clusters data into similar groups. The algorithm predicts values of new data to replace NA values based on how closely they resembles training data points, such as by comparing across other columns.\nHere’s a visual example using the clara() function from the cluster package to run a KNN algorithm on our dataset, where three clusters are created by the algorithm.\n\n# KNN plot of our dataset without categories\nautoplot(clara(dataset[-5], 3)) +\n  scale_color_OkabeIto()\n\n\n\n\n\n# Raw summary, output suppressed\nknn_na_imp_insulin &lt;- na.dataset |&gt;\n  imputate_na(Insulin, method = \"knn\")\n\n# Plot showing the results of our imputation\nknn_na_imp_insulin |&gt;\n  plot()\n\n\n\n\n\n\nPros & Cons of Using KNN Imputation\nPro:\n\nPossibly much more accurate than mean, median, or mode imputation for some data sets.\n\nCons:\n\nKNN is computationally expensive because it stores the entire training dataset into computer memory.\nKNN is very sensitive to outliers, so you would have to imputate these first.\n\n\n\n\n\nRecursive Partitioning and Regression Trees (rpart)\nrpart is a decision tree machine learning algorithm that builds classification or regression models through a two stage process, which can be thought of as binary trees. The algorithm splits the data into subsets, which move down other branches of the tree until a termination criteria is reached.\nFor example, if we are missing a value for Age_group a first decision could be whether the associated Age is within a series of yes or no criteria\n\n\n\n\n\n\n# Raw summary, output suppressed\nrpart_na_imp_insulin &lt;- na.dataset |&gt;\n  imputate_na(Insulin, method = \"rpart\")\n\n# Plot showing the results of our imputation\nrpart_na_imp_insulin |&gt;\n  plot()\n\n\n\n\n\n\nPros & Cons of Using rpart Imputation\nPros:\n\nGood for categorical data because approximations are easier to compare across categories than continuous variables.\nNot sensitive to outliers.\n\nCons:\n\nCan over fit the data as they grow.\nSpeed decreases with more data columns.\n\n\n\n\n\nMultivariate Imputation by Chained Equations (MICE)\nMICE is an algorithm that fills missing values multiple times, hence dealing with uncertainty better than other methods. This approach creates multiple copies of the data that can then be analyzed and then pooled into a single dataset.\nNOTE: You will have to set a random seed (e.g., 123) since the MICE algorithm pools several simulated imputations. Without setting a seed, a different result will occur after each simulation.\n\n\n\nImage Credit: Will Badr\n\n\n\n# Raw summary, output suppressed\nmice_na_imp_insulin &lt;- na.dataset |&gt;\n  imputate_na(Insulin, method = \"mice\", seed = 123)\n\n\n iter imp variable\n  1   1  Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin  BMI  DiabetesPedigreeFunction  Age  Outcome  Age_group\n  1   2  Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin  BMI  DiabetesPedigreeFunction  Age  Outcome  Age_group\n  1   3  Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin  BMI  DiabetesPedigreeFunction  Age  Outcome  Age_group\n  1   4  Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin  BMI  DiabetesPedigreeFunction  Age  Outcome  Age_group\n  1   5  Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin  BMI  DiabetesPedigreeFunction  Age  Outcome  Age_group\n  2   1  Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin  BMI  DiabetesPedigreeFunction  Age  Outcome  Age_group\n  2   2  Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin  BMI  DiabetesPedigreeFunction  Age  Outcome  Age_group\n  2   3  Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin  BMI  DiabetesPedigreeFunction  Age  Outcome  Age_group\n  2   4  Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin  BMI  DiabetesPedigreeFunction  Age  Outcome  Age_group\n  2   5  Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin  BMI  DiabetesPedigreeFunction  Age  Outcome  Age_group\n  3   1  Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin  BMI  DiabetesPedigreeFunction  Age  Outcome  Age_group\n  3   2  Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin  BMI  DiabetesPedigreeFunction  Age  Outcome  Age_group\n  3   3  Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin  BMI  DiabetesPedigreeFunction  Age  Outcome  Age_group\n  3   4  Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin  BMI  DiabetesPedigreeFunction  Age  Outcome  Age_group\n  3   5  Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin  BMI  DiabetesPedigreeFunction  Age  Outcome  Age_group\n  4   1  Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin  BMI  DiabetesPedigreeFunction  Age  Outcome  Age_group\n  4   2  Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin  BMI  DiabetesPedigreeFunction  Age  Outcome  Age_group\n  4   3  Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin  BMI  DiabetesPedigreeFunction  Age  Outcome  Age_group\n  4   4  Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin  BMI  DiabetesPedigreeFunction  Age  Outcome  Age_group\n  4   5  Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin  BMI  DiabetesPedigreeFunction  Age  Outcome  Age_group\n  5   1  Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin  BMI  DiabetesPedigreeFunction  Age  Outcome  Age_group\n  5   2  Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin  BMI  DiabetesPedigreeFunction  Age  Outcome  Age_group\n  5   3  Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin  BMI  DiabetesPedigreeFunction  Age  Outcome  Age_group\n  5   4  Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin  BMI  DiabetesPedigreeFunction  Age  Outcome  Age_group\n  5   5  Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin  BMI  DiabetesPedigreeFunction  Age  Outcome  Age_group\n\n# Plot showing the results of our imputation\nmice_na_imp_insulin |&gt;\n  plot()\n\n\n\n\n\n\nPros & Cons of MICE Imputation\nPros:\n\nMultiple imputations are more accurate than a single imputation.\nThe chained equations are very flexible to data types, such as categorical and ordinal.\n\nCons:\n\nYou have to round the results for ordinal data because resulting data points are too great or too small (floating-points)."
  },
  {
    "objectID": "ImputingLikeDataScientist.html#produce-an-html-transformation-summary",
    "href": "ImputingLikeDataScientist.html#produce-an-html-transformation-summary",
    "title": "Imputing like a Data Scientist",
    "section": "Produce an HTML Transformation Summary",
    "text": "Produce an HTML Transformation Summary\n\ntransformation_web_report(dataset)"
  },
  {
    "objectID": "CorrelateLikeDataMaster.html#purpose-of-this-chapter",
    "href": "CorrelateLikeDataMaster.html#purpose-of-this-chapter",
    "title": "Correlating Like a Data Master",
    "section": "Purpose of this chapter",
    "text": "Purpose of this chapter\nAssess relationships within a novel data set using publication quality tables and plots"
  },
  {
    "objectID": "CorrelateLikeDataMaster.html#take-aways",
    "href": "CorrelateLikeDataMaster.html#take-aways",
    "title": "Correlating Like a Data Master",
    "section": "Take-aways",
    "text": "Take-aways\n\nDescribe and visualize correlations between numerical variables\nVisualize correlations of all numerical variables within groups\nDescribe and visualize relationships based on target variables\n\n\n\nRequired setup\nWe first need to prepare our environment with the necessary packages.\n\noptions(repos = list(CRAN = \"http://cran.rstudio.com/\"))\n\ninstall.packages(\"pacman\")\n\n\nThe downloaded binary packages are in\n    /var/folders/61/5w0zfjkx2ks_c31ggc0f8gch0000gt/T//RtmpwvyB0z/downloaded_packages\n\npacman::p_load(colorblindr,\n       dlookr,\n       formattable,\n       GGally,\n       ggdist,\n       ggpubr,\n       ggridges,\n       here,\n       tidyverse)\n\n# Set global ggplot() theme\n# Theme pub_clean() from the ggpubr package with base text size = 16\ntheme_set(theme_pubclean(base_size = 12)) \n# All axes titles to their respective far right sides\ntheme_update(axis.title = element_text(hjust = 1))\n# Remove axes ticks\ntheme_update(axis.ticks = element_blank()) \n# Remove legend key\ntheme_update(legend.key = element_blank())"
  },
  {
    "objectID": "CorrelateLikeDataMaster.html#load-the-examine-a-data-set",
    "href": "CorrelateLikeDataMaster.html#load-the-examine-a-data-set",
    "title": "Correlating Like a Data Master",
    "section": "Load the Examine a Data Set",
    "text": "Load the Examine a Data Set\nWe will be using open source data from UArizona researchers that investigates the effects of climate change on canopy trees. (Meredith, Ladd, and Werner 2021)\n\n# Let's load the canopy tree data set\ndataset &lt;- read.csv(here(\"EDA_In_R_Book\", \"data\", \"Data_Fig2_Repo.csv\"))\n\n# What does the data look like?\ndataset |&gt;\n  head() |&gt;\n  formattable()\n\n\n\n\n\n\nDate\n\n\nGroup\n\n\nSap_Flow\n\n\nTWaterFlux\n\n\npLWP\n\n\nmLWP\n\n\n\n\n\n\n10/4/19\n\n\nDrought-sens-canopy\n\n\n184.040975\n\n\n82.243292\n\n\n-0.2633781\n\n\n-0.6797690\n\n\n\n\n10/4/19\n\n\nDrought-sens-under\n\n\n2.475989\n\n\n1.258050\n\n\n-0.2996688\n\n\n-0.7613264\n\n\n\n\n10/4/19\n\n\nDrought-tol-canopy\n\n\n10.598949\n\n\n4.405479\n\n\n-0.4375563\n\n\n-0.7225572\n\n\n\n\n10/4/19\n\n\nDrought-tol-under\n\n\n4.399854\n\n\n2.055276\n\n\n-0.2052237\n\n\n-0.7028581\n\n\n\n\n10/5/19\n\n\nDrought-sens-canopy\n\n\n182.905444\n\n\n95.865255\n\n\n-0.2769280\n\n\n-0.7082610\n\n\n\n\n10/5/19\n\n\nDrought-sens-under\n\n\n2.459209\n\n\n1.225792\n\n\n-0.3205980\n\n\n-0.7928576"
  },
  {
    "objectID": "CorrelateLikeDataMaster.html#describe-and-visualize-correlations",
    "href": "CorrelateLikeDataMaster.html#describe-and-visualize-correlations",
    "title": "Correlating Like a Data Master",
    "section": "Describe and Visualize Correlations",
    "text": "Describe and Visualize Correlations\nCorrelations are a statistical relationship between two numerical variables, may or may not be causal. Exploring correlations in your data allows you determine data independence, a major assumption of parametric statistics, which means your variables are both randomly collected.\n\nIf you’re interested in some underlying statistics…\nNote that the dlookr default correlation is the Pearson’s \\(r\\) coefficient, but you can specify any method you would like: correlate(dataset, method = \"\"), where the method can be \"pearson\" for Pearson’s \\(r\\), \"spearman\" for Spearman’s \\(\\rho\\), or \"kendall\" for Kendall’s \\(\\tau\\). The main differences are that Pearson’s \\(r\\) assumes a normal distribution for ALL numerical variables, whereas Spearman’s \\(\\rho\\) and Kendall’s \\(\\tau\\) do not, but Spearman’s \\(\\rho\\) requires \\(N &gt; 10\\), and Kendall’s \\(\\tau\\) does not. Notably, Kendall’s \\(\\tau\\) performs as well as Spearman’s \\(\\rho\\) when \\(N &gt; 10\\), so its best to just use Kendall’s \\(\\tau\\) when data are not normally distributed.\n\n# Table of correlations between numerical variables (we are sticking to the default Pearson's r coefficient)\ncorrelate(dataset) |&gt;\n  formattable()\n\n\n\n\n\n\nvar1\n\n\nvar2\n\n\ncoef_corr\n\n\n\n\n\n\nTWaterFlux\n\n\nSap_Flow\n\n\n0.9881370\n\n\n\n\npLWP\n\n\nSap_Flow\n\n\n0.1202809\n\n\n\n\nmLWP\n\n\nSap_Flow\n\n\n-0.2011949\n\n\n\n\nSap_Flow\n\n\nTWaterFlux\n\n\n0.9881370\n\n\n\n\npLWP\n\n\nTWaterFlux\n\n\n0.1256446\n\n\n\n\nmLWP\n\n\nTWaterFlux\n\n\n-0.1893302\n\n\n\n\nSap_Flow\n\n\npLWP\n\n\n0.1202809\n\n\n\n\nTWaterFlux\n\n\npLWP\n\n\n0.1256446\n\n\n\n\nmLWP\n\n\npLWP\n\n\n0.6776509\n\n\n\n\nSap_Flow\n\n\nmLWP\n\n\n-0.2011949\n\n\n\n\nTWaterFlux\n\n\nmLWP\n\n\n-0.1893302\n\n\n\n\npLWP\n\n\nmLWP\n\n\n0.6776509\n\n\n\n\n\n\n\n\n# Correlation matrix of numerical variables\ndataset |&gt;\nplot_correlate()"
  },
  {
    "objectID": "CorrelateLikeDataMaster.html#visualize-correlations-within-groups",
    "href": "CorrelateLikeDataMaster.html#visualize-correlations-within-groups",
    "title": "Correlating Like a Data Master",
    "section": "Visualize Correlations within Groups",
    "text": "Visualize Correlations within Groups\nIf we have groups that we will compare later on, it is a good idea to see how each numerical variable correlates within these groups.\n\ndataset |&gt;\n  group_by(Group) |&gt;\n  plot_correlate()\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is great, we have our correlations within groups! However, the correlation matrices aren’t always the most intuitive, so let’s plot!\nWe will be using the ggpairs() function within the GGally package. Specifically, we are looking at the correlations between predawn leaf water potential pLWP and midday leaf water potential mLWP. Leaf water potential is a key indicator for how stressed plants are in droughts.\n\ndataset |&gt; \n  dplyr::select(Group, pLWP, mLWP) |&gt;\n  ggpairs(aes(color = Group, alpha = 0.5)) +\n  theme(strip.background = element_blank()) + # I don't like facet strip backgrounds...\n  scale_fill_OkabeIto() +\n  scale_color_OkabeIto()"
  },
  {
    "objectID": "CorrelateLikeDataMaster.html#describe-and-visualize-relationships-based-on-target-variables",
    "href": "CorrelateLikeDataMaster.html#describe-and-visualize-relationships-based-on-target-variables",
    "title": "Correlating Like a Data Master",
    "section": "Describe and Visualize Relationships Based on Target Variables",
    "text": "Describe and Visualize Relationships Based on Target Variables\n\nTarget Variables\nTarget variables are essentially numerical or categorical variables that you want to relate others to in a data frame. dlookr does this through the target_by() function, which is similar to group_by() in dplyr. The relate() function then briefly analyzes the relationship between the target variable and the variables of interest.\nThe relationships below will have the formula relationship target ~ predictor.\n\n\n\nNumerical Target Variables: Numerical Variable of Interest\nFormula: Sap_Flow (numerical response)  ~ pLWP (numerical predictor)\n\n# First, we need to remove NAs, they cause an error\ndataset.noNA &lt;- dataset |&gt; \n  drop_na()\n\n# The numerical predictor variable that we want\nnum &lt;- target_by(dataset.noNA, Sap_Flow)\n\n# Relating the variable of interest to the numerical target variable\nnum_num &lt;- relate(num, pLWP)\n\n# Summary of the regression analysis - the same as the summary from lm(Formula)\nsummary(num_num)\n\n\nCall:\nlm(formula = formula_str, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.719 -26.647 -20.924  -0.811 148.353 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   42.167      7.971   5.290 2.51e-07 ***\npLWP          24.600     12.266   2.006   0.0459 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 46.2 on 274 degrees of freedom\nMultiple R-squared:  0.01447,   Adjusted R-squared:  0.01087 \nF-statistic: 4.022 on 1 and 274 DF,  p-value: 0.04589\n\n\n\n# Plotting the linear relationship\nplot(num_num)\n\n\n\n\n\n\n\nNumerical Target Variables: Categorical Variable of Interest\nFormula: pLWP (numerical response) ~ Group (categorical predictor)\n\n# The categorical predictor variable that we want\nnum &lt;- target_by(dataset, pLWP) \n\n# We need to change Group to a factor\nnum$Group &lt;- as.factor(num$Group)\n\n# Relating the variable of interest to the numerical target variable\nnum_cat &lt;- relate(num, Group)\n\n# Summary of the ANOVA analysis - the same as the summary from anova(lm(Formula))\nsummary(num_cat)\n\n\nCall:\nlm(formula = formula(formula_str), data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.73720 -0.09700  0.03598  0.11314  0.40655 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             -0.66993    0.02466 -27.166  &lt; 2e-16 ***\nGroupDrought-sens-under -0.02621    0.03488  -0.751    0.453    \nGroupDrought-tol-canopy  0.04002    0.03488   1.148    0.252    \nGroupDrought-tol-under   0.22969    0.03488   6.586 2.33e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2048 on 272 degrees of freedom\n  (312 observations deleted due to missingness)\nMultiple R-squared:  0.1956,    Adjusted R-squared:  0.1867 \nF-statistic: 22.05 on 3 and 272 DF,  p-value: 8.267e-13\n\n\n\nplot(num_cat) + \n  theme(axis.text.x = element_blank())\n\n\n\n\n\n\n\nCategorical Target Variables: Numerical Variable of Interest\nNote that this produces descriptive statistics, unlike the other relationships we are looking at.\nFormula: Group (categorical) ~ pLWP (numerical)\n\n# The categorical predictor variable that we want\ncateg &lt;- target_by(dataset, Group)\n\n# Relating the variable of interest to the numerical target variable\ncat_num &lt;- relate(categ, pLWP)\n\n# Summary of descriptive statistics\nsummary(cat_num)\n\n described_variables    Group                 n               na       \n Length:5            Length:5           Min.   : 69.0   Min.   : 78.0  \n Class :character    Class :character   1st Qu.: 69.0   1st Qu.: 78.0  \n Mode  :character    Mode  :character   Median : 69.0   Median : 78.0  \n                                        Mean   :110.4   Mean   :124.8  \n                                        3rd Qu.: 69.0   3rd Qu.: 78.0  \n                                        Max.   :276.0   Max.   :312.0  \n      mean               sd             se_mean             IQR        \n Min.   :-0.6961   Min.   :0.09557   Min.   :0.01151   Min.   :0.1351  \n 1st Qu.:-0.6699   1st Qu.:0.13188   1st Qu.:0.01367   1st Qu.:0.1597  \n Median :-0.6299   Median :0.22715   Median :0.01588   Median :0.2640  \n Mean   :-0.6091   Mean   :0.19699   Mean   :0.02098   Mean   :0.2310  \n 3rd Qu.:-0.6091   3rd Qu.:0.24639   3rd Qu.:0.02966   3rd Qu.:0.2788  \n Max.   :-0.4402   Max.   :0.28394   Max.   :0.03418   Max.   :0.3173  \n    skewness          kurtosis            p00               p01         \n Min.   :-1.1510   Min.   :-0.5921   Min.   :-1.4333   Min.   :-1.4307  \n 1st Qu.:-1.1047   1st Qu.:-0.4320   1st Qu.:-1.4333   1st Qu.:-1.3160  \n Median :-0.4644   Median :-0.2744   Median :-1.2993   Median :-1.2538  \n Mean   :-0.7009   Mean   : 0.1897   Mean   :-1.1553   Mean   :-1.1133  \n 3rd Qu.:-0.4333   3rd Qu.: 0.4796   3rd Qu.:-0.8637   3rd Qu.:-0.8388  \n Max.   :-0.3510   Max.   : 1.7675   Max.   :-0.7467   Max.   :-0.7272  \n      p05               p10               p20               p25         \n Min.   :-1.3067   Min.   :-1.1449   Min.   :-0.9067   Min.   :-0.8000  \n 1st Qu.:-1.0870   1st Qu.:-1.0047   1st Qu.:-0.8587   1st Qu.:-0.7906  \n Median :-1.0666   Median :-0.8802   Median :-0.7339   Median :-0.7140  \n Mean   :-0.9838   Mean   :-0.8878   Mean   :-0.7583   Mean   :-0.7063  \n 3rd Qu.:-0.7816   3rd Qu.:-0.7724   3rd Qu.:-0.7304   3rd Qu.:-0.7065  \n Max.   :-0.6772   Max.   :-0.6366   Max.   :-0.5619   Max.   :-0.5205  \n      p30               p40               p50               p60         \n Min.   :-0.7386   Min.   :-0.7204   Min.   :-0.7059   Min.   :-0.6206  \n 1st Qu.:-0.7076   1st Qu.:-0.6299   1st Qu.:-0.6028   1st Qu.:-0.5861  \n Median :-0.6801   Median :-0.6292   Median :-0.5921   Median :-0.5855  \n Mean   :-0.6590   Mean   :-0.6079   Mean   :-0.5787   Mean   :-0.5487  \n 3rd Qu.:-0.6790   3rd Qu.:-0.6171   3rd Qu.:-0.5862   3rd Qu.:-0.5536  \n Max.   :-0.4896   Max.   :-0.4427   Max.   :-0.4064   Max.   :-0.3978  \n      p70               p75               p80               p90         \n Min.   :-0.5758   Min.   :-0.5714   Min.   :-0.5653   Min.   :-0.5150  \n 1st Qu.:-0.5504   1st Qu.:-0.5212   1st Qu.:-0.4907   1st Qu.:-0.4243  \n Median :-0.5269   Median :-0.4733   Median :-0.4259   Median :-0.3519  \n Mean   :-0.5068   Mean   :-0.4753   Mean   :-0.4446   Mean   :-0.3812  \n 3rd Qu.:-0.4925   3rd Qu.:-0.4500   3rd Qu.:-0.4088   3rd Qu.:-0.3394  \n Max.   :-0.3883   Max.   :-0.3608   Max.   :-0.3325   Max.   :-0.2756  \n      p95               p99               p100        \n Min.   :-0.5100   Min.   :-0.4609   Min.   :-0.4376  \n 1st Qu.:-0.3700   1st Qu.:-0.3139   1st Qu.:-0.2997  \n Median :-0.3049   Median :-0.2726   Median :-0.2634  \n Mean   :-0.3457   Mean   :-0.2994   Mean   :-0.2822  \n 3rd Qu.:-0.3004   3rd Qu.:-0.2363   3rd Qu.:-0.2052  \n Max.   :-0.2430   Max.   :-0.2133   Max.   :-0.2052  \n\n\n\nplot(cat_num) \n\n\n\n\n\n\n\nCategorical Target Variables: Categorical Variable of Interest\nNotably, there is only one categorical variable… Let’s make another:\nIf \\(mLWP &gt; mean(mLWP) + sd(mLWP)\\) then “Yes”, else “No”.\n\n# Create new categorical column\ncat_dataset &lt;- dataset |&gt;\n  select(pLWP, Group) |&gt;\n  drop_na() |&gt;\n  mutate(HighLWP = ifelse(\n    pLWP &gt; (mean(pLWP + sd(pLWP))), \n                          \"Yes\", \n                          \"No\"))\n\n# New dataset \ncat_dataset |&gt;\n  head() |&gt;\n  formattable()\n\n\n\n\n\n\npLWP\n\n\nGroup\n\n\nHighLWP\n\n\n\n\n\n\n-0.2633781\n\n\nDrought-sens-canopy\n\n\nYes\n\n\n\n\n-0.2996688\n\n\nDrought-sens-under\n\n\nYes\n\n\n\n\n-0.4375563\n\n\nDrought-tol-canopy\n\n\nNo\n\n\n\n\n-0.2052237\n\n\nDrought-tol-under\n\n\nYes\n\n\n\n\n-0.2769280\n\n\nDrought-sens-canopy\n\n\nYes\n\n\n\n\n-0.3205980\n\n\nDrought-sens-under\n\n\nYes\n\n\n\n\n\n\n\nNow we have two categories!\nFormula = HighLWP (categorical) ~ Group (categorical response)\n\n# The categorical predictor variable that we want\ncateg &lt;- target_by(cat_dataset, HighLWP)\n\n# Relating the variable of interest to the categorical target variable\ncat_cat &lt;- relate(categ, Group)\n\n# Summary of the Chi-square test for Independence\nsummary(cat_cat)\n\nCall: xtabs(formula = formula_str, data = data, addNA = TRUE)\nNumber of cases in table: 276 \nNumber of factors: 2 \nTest for independence of all factors:\n    Chisq = 30.201, df = 3, p-value = 1.252e-06\n\n\n\nplot(cat_cat)\n\n\n\n\n\n\n\n\n\nMeredith, Laura, S. Nemiah Ladd, and Christiane Werner. 2021. “Data for \"Ecosystem Fluxes During Drought and Recovery in an Experimental Forest\".” University of Arizona Research Data Repository. https://doi.org/10.25422/AZU.DATA.14632593.V1."
  }
]