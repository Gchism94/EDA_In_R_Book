# Exploratory Data Analysis in R - Imputating like a Data Scientist

```{r setup, include = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

## Purpose of Workshop

**Exploring, visualizing, and imputing outliers and missing values (NAs) in a novel data set and produce publication quality graphs and tables**

------------------------------------------------------------------------

## Objectives

1.  Load and explore a data set with publication quality tables
2.  Thoroughly diagnose outliers and missing values
3.  Impute outliers and missing values

------------------------------------------------------------------------

## Required setup

We first need to prepare our environment with the necessary packages and set a global theme for publishable plots in `ggplot()`

```{r}
options(repos = list(CRAN = "http://cran.rstudio.com/"))
options(digits = 2)

install.packages("pacman")

pacman::p_load(colorblindr, # Colorblind friendly pallettes
               cluster, # K cluster analyses
               dlookr, # Exploratory data analysis
               formattable, # HTML tables from R outputs
               ggfortify, # Plotting tools for stats
               ggpubr, # Publishable ggplots
               here, # Standardizes paths to data
               kableExtra, # Alternative to formattable
               knitr, # Needed to write HTML reports
               missRanger, # To generate NAs
               plotly, # Visualization package
               rattle, # Decision tree visualization
               rpart, # rpart algorithm
               tidyverse, # Powerful data wrangling package suite
               visdat) # Another EDA visualization package

theme_set(theme_pubclean(base_size = 16))
theme_update(axis.title = element_text(hjust = 1))
theme_update(axis.ticks = element_blank()) 
theme_update(legend.key = element_blank())
```

------------------------------------------------------------------------

### Load and Examine a Data Set

```{r}
# Let's load a data set from the flights data set
dataset <- read.csv(here("EDA_In_R_Summer_Book", "data", "diabetes.csv")) |>
  # Add a categorical group
  mutate(Age_group = ifelse(Age >= 21 & Age <= 30, "Young", 
                            ifelse(Age > 30 & Age <=50, "Middle", 
                                   "Elderly")),
         Age_group = fct_rev(Age_group))

# What does the data look like?
dataset |>
  head() |>
  formattable()
```

------------------------------------------------------------------------

## Diagnose your data

```{r}
# What are the properties of the data
dataset |>
  diagnose() |>
  formattable()
```

-   `variables`: name of each variable
-   `types`: data type of each variable
-   `missing_count`: number of missing values
-   `missing_percent`: percentage of missing values
-   `unique_count`: number of unique values
-   `unique_rate`: rate of unique value - unique_count / number of observations

------------------------------------------------------------------------

## Diagnose Outliers

There are several numerical variables that have outliers above, let's see what the data look like with and without them

-   Create a table with columns containing outliers

-   Plot outliers in a box plot and histogram

```{r}
# Table showing outliers
dataset |>
  diagnose_outlier() |>
  filter(outliers_ratio > 0) |>  
  mutate(rate = outliers_mean / with_mean) |>
  arrange(desc(rate)) |> 
  select(-outliers_cnt)
```

```{r}
# Boxplots and histograms of data with and without outliers
dataset |>
  select(find_outliers(dataset)) |>
           plot_outlier()
```

------------------------------------------------------------------------

## Basic exploration of missing values (NAs)

-   Table showing the extent of NAs in columns containing them

```{r, output.width = "100%"}
# Randomly generate NAs for 30
na.dataset <- dataset |>
  generateNA(p = 0.3)

na.dataset |>
head() |>
  formattable()

# Create the NA table
na.dataset |>
  plot_na_pareto(only_na = TRUE, plot = FALSE) |>
  formattable() # Publishable table
```

-   Plots showing the frequency of missing values

```{r}
# Plot the insersect of the columns with missing values
# This plot visualizes the table above
na.dataset |>
  plot_na_pareto(only_na = TRUE)
```

------------------------------------------------------------------------

## Advanced Exploration of Missing Values (NAs)

-   Intersect plot that shows, for every combination of columns relevant, how many missing values are common
-   Orange boxes are the columns in question
-   x axis (top green bar plots) show the number of missing values in that column
-   y axis (right green bars) show the number of missing values in the columns in orange blocks

```{r}
# Plot the insersect of the 5 columns with the most missing values
# This means that some combinations of columns have missing values in the same row
na.dataset |>
  select(BloodPressure, Glucose, Age) |>
  plot_na_intersect(only_na = TRUE) 
```

------------------------------------------------------------------------

### Determining if NA Observations are the Same

-   Missing values can be the same observation across several columns, this is not shown above
-   The visdat package can solve this with the `vis_miss()` function which shows the rows with missing values through `ggplotly()`
-   Here we will show ALL columns with NAs, and you can zoom into individual rows (interactive plot)
-   NOTE: This line will make the HTML rendering take a while...

```{r}
na.dataset |>
 select(BloodPressure, Glucose, Age) |>
 vis_miss() |>
 ggplotly() 
```

------------------------------------------------------------------------

## Impute outliers and NAs

Removing outliers and NAs can be tricky, but there are methods to do so. I will go over several, and discuss benefits and costs to each.

The principle goal for all imputation is to find the method that does not change the distribution too much (or oddly).

------------------------------------------------------------------------

#### NOTE: imputation should only be used when missing data is unavoidable and probably limited to 10% of your data being outliers / missing data (though some argue imputation is necessary between 30-60%). Ask what the cause is for the outlier and missing data.

------------------------------------------------------------------------

### Classifying outliers

Before imputing outliers, you will want to diagnose whether it's they are natural outliers or not. We will not be looking at "Insulin" for example across Age_group, because there are several NAs, which we will impute below.

```{r}
# Box plot
dataset %>% # Set the simulated normal data as a data frame
  ggplot(aes(x = Insulin, y = Age_group, fill = Age_group)) + # Create a ggplot
  geom_boxplot(width = 0.5, outlier.size = 2, outlier.alpha = 0.5) +
  xlab("Insulin (mg/dL)") +  # Relabel the x axis label
  ylab("Age group") + # Remove the y axis label
  scale_fill_OkabeIto() + # Change the color scheme for the fill criteria
  theme(legend.position = "none")  # Remove the legend 
```

Now let's say that we want to impute extreme values and remove outliers that don't make sense, such as Insulin levels \> 600 mg/dL: values greater than this induce a diabetic coma.

We remove outliers using `imputate_outlier()` and replace them with values that are estimates based on the existing data

-   `mean`: arithmetic mean

-   `median`: median

-   `mode`: mode

-   `capping`: Impute the upper outliers with 95 percentile, and impute the bottom outliers with 5 percentile - aka Winsorizing

### Mean imputation

The mean of the observed values for each variable is computed and the outliers for that variable are imputed by this mean

```{r}
# Raw summary, output suppressed
mean_out_imp_insulin <- dataset |>
  select(Insulin) |>
  filter(Insulin < 600) |>
  imputate_outlier(Insulin, method = "mean")

mean_out_imp_insulin |>
  summary() |>
  kable() |>
  kable_styling()
```

```{r}
# Visualization of the mean imputation
mean_out_imp_insulin |>
  plot()
```

### Median imputation

The median of the observed values for each variable is computed and the outliers for that variable are imputed by this median

```{r}
# Raw summary, output suppressed
med_out_imp_insulin <- dataset |>
  select(Insulin) |>
  filter(Insulin < 600) |>
  imputate_outlier(Insulin, method = "median")

med_out_imp_insulin |>
  summary() |>
  kable() |>
  kable_styling()
```

```{r}
# Visualization of the median imputation
med_out_imp_insulin |>
  plot()
```

------------------------------------------------------------------------

#### Pros & cons of using the mean or median imputation

**Pros**:

-   Easy and fast.
-   Works well with small numerical datasets.

**Cons**:

-   Doesn't factor the correlations between features. It only works on the column level.
-   Will give poor results on encoded categorical features (do **NOT** use it on categorical features).
-   Not very accurate.
-   Doesn't account for the uncertainty in the imputations.

------------------------------------------------------------------------

### Mode imputation

The mode of the observed values for each variable is computed and the outliers for that variable are imputed by this mode

```{r}
# Raw summary, output suppressed
mode_out_imp_insulin <- dataset |>
  select(Insulin) |>
  filter(Insulin < 600) |>
  imputate_outlier(Insulin, method = "mode")

mode_out_imp_insulin |>
  summary() |>
  kable() |>
  kable_styling()
```

```{r}
# Visualization of the mode imputation
mode_out_imp_insulin |>
plot()
```

------------------------------------------------------------------------

#### Pros & cons of using the mean / median imputation

**Pros**:

-   Works well with categorical features.

**Cons**:

-   It also doesn't factor the correlations between features.

-   It can introduce bias in the data.

------------------------------------------------------------------------

### Capping imputation (aka Winsorizing)

The Percentile Capping is a method of Imputing the outlier values by replacing those observations outside the lower limit with the value of 5th percentile and those that lie above the upper limit, with the value of 95th percentile of the same dataset.

```{r}
# Raw summary, output suppressed
cap_out_imp_insulin <- dataset |>
  select(Insulin) |>
  filter(Insulin < 600) |>
  imputate_outlier(Insulin, method = "capping")

cap_out_imp_insulin |>
  summary() |>
  kable() |>
  kable_styling()
```

```{r}
# Visualization of the capping imputation
cap_out_imp_insulin |>
  plot()
```

------------------------------------------------------------------------

#### Pros and cons of capping

**Pros**:

-   Not influenced by extreme values

**Cons**:

-   Capping only modifies the smallest and largest values slightly. This is generally not a good idea since it means we're just modifying data values for the sake of modifications.

-   If no extreme outliers are present, Winsorization may be unnecessary.

------------------------------------------------------------------------

## Imputing NAs

I will only be addressing one type of NA imputation using `imputate_na()` (but note you can use mean, median, and mode as well):

1.  `knn`: K-nearest neighbors (KNN)
2.  `rpart`: Recursive Partitioning and Regression Trees (rpart)
3.  `mice`: Multivariate Imputation by Chained Equations (MICE)

Since our normal `dataset` has no NA values, we will use the `na.dataset` we created earlier.

### K-Nearest Neighbor (KNN) imputation

KNN is a machine learning algorithm that classifies data by similarity. This in effect clusters data into similar groups. The algorithm predicts values of new data to replace NA values based on how closely they resembles training data points, such as by comparing across other columns.

Here's a visual example using the `clara()` function from the `cluster` package to run a KNN algorithm on our `dataset`, where three clusters are created by the algorithm.

```{r}
autoplot(clara(dataset[-5], 3)) +
  scale_color_OkabeIto()

```

```{r}
# Raw summary, output suppressed
knn_na_imp_insulin <- na.dataset |>
  imputate_na(Insulin, method = "knn")

knn_na_imp_insulin |>
  plot()
```

#### Pros & cons of using KNN imputation

**Pro**:

-   Possibly much more accurate than mean, median, or mode imputation for some data sets.

**Cons**:

-   KNN is computationally expensive because it stores the entire training dataset into computer memory.

-   KNN is very sensitive to outliers, so you would have to imputate these first.

### Recursive Partitioning and Regression Trees (rpart)

rpart is a decision tree machine learning algorithm that builds classification or regression models through a two stage process, which can be thought of as binary trees. The algorithm splits the data into subsets, which move down other branches of the tree until a termination criteria is reached.

For example, if we are missing a value for `Age_group` a first decision could be whether the associated `Age` is within a series of yes or no criteria

```{r, echo = FALSE}

fancyRpartPlot(rpart(Age_group~., data = na.dataset),
               yesno = 2,
               split.col="black",
               nn.col="black", 
               caption="",
               palette="Set2",
               branch.col="black")
```

```{r}
# Raw summary, output suppressed
rpart_na_imp_insulin <- na.dataset |>
  imputate_na(Insulin, method = "rpart")

rpart_na_imp_insulin |>
  plot()
```

#### Pros & cons of using rpart imputation

**Pros**:

-   Good for categorical data because approximations are easier to compare across categories than continuous variables.

-   Not sensitive to outliers.

**Cons**:

-   Can overfit the data as they grow.

-   Speed decreases with more data columns.

###  Multivariate Imputation by Chained Equations (MICE)

MICE is an algorithm that fills missing values multiple times, hence dealing with uncertainty better than other methods. This approach creates multiple copies of the data that can then be analyzed and then pooled into a single dataset.

![Image Credit: [Will Badr](https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779)](https://miro.medium.com/max/1400/1*cmZFWypJUrFL2QL3KyzXEQ.png)

```{r}
# Raw summary, output suppressed
mice_na_imp_insulin <- na.dataset |>
  imputate_na(Insulin, method = "mice")

mice_na_imp_insulin |>
  plot()
```

#### Pros & cons of MICE imputation
